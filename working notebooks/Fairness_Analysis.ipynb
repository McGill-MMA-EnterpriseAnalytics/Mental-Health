{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsCnwN-MopN_"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP5nu8FlogFH"
   },
   "source": [
    "This notebook conducts a **fairness analysis** on a machine learning model trained to predict mental health treatment needs based on survey data. The model aims to classify whether individuals are likely to seek or receive treatment (treatment as the target). Fairness evaluation is performed across key sensitive attributes, including **Gender, Age Group, Self-Employment Status, and Family History**, to ensure equitable prediction outcomes across diverse groups. The analysis at the end identifies potential biases in model predictions and highlights groups that may be at risk of under- or over-prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHPMBcuypBwt"
   },
   "source": [
    "This notebook conducts a fairness analysis on a machine learning model trained to predict mental health treatment needs (treatment) based on survey data.\n",
    "We evaluated the model's fairness across key sensitive attributes — Gender, Age Group, Self-Employment Status, and Family History — using metrics such as Disparate Impact (DI), Equalized Odds Difference (EOD), and True Positive Rate (TPR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCzehA1FleIg"
   },
   "source": [
    "## Model and Data Preparation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bTAfp1CowR4k",
    "outputId": "93bfd1a4-7a78-4d52-a1a9-e9601678da14"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path = '/content/drive/MyDrive/survey.csv'\n",
    "df = pd.read_csv(path, encoding='latin1')\n",
    "\n",
    "df = df.drop(columns=['comments', 'Timestamp', 'state', 'no_employees', 'anonymity'])\n",
    "df = df[(df['Age'] >= 18) & (df['Age'] <= 100)]\n",
    "df['self_employed'] = df['self_employed'].fillna('No')\n",
    "df.drop_duplicates()\n",
    "df['work_interfere'] = df['work_interfere'].fillna('Not applicable')\n",
    "\n",
    "#Function to clean the gender variable into 'female', 'male', and 'other'\n",
    "def clean_gender(gender):\n",
    "    gender = str(gender).strip().lower()\n",
    "    if gender in ['male', 'm', 'cis male', 'male (cis)', 'man', 'mail', 'cis man', 'malr', 'make', 'maile']:\n",
    "        return 'Male'\n",
    "    elif gender in ['female', 'f', 'cis female', 'woman', 'female (cis)', 'cis-female/femme', 'femake', 'femail', 'female ', 'trans female']:\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the cleaning function\n",
    "df['Gender'] = df['Gender'].apply(clean_gender)\n",
    "\n",
    "for col in ['Gender','Country','self_employed','family_history','work_interfere','remote_work','tech_company','benefits','care_options',\n",
    "            'wellness_program','seek_help','leave','mental_health_consequence','phys_health_consequence','coworkers','supervisor',\n",
    "            'mental_health_interview','phys_health_interview','mental_vs_physical','obs_consequence']:\n",
    "            vc = df[col].value_counts(normalize=True)\n",
    "            rare = vc[vc < 0.01]\n",
    "\n",
    "vc = df['Country'].value_counts(normalize=True)\n",
    "rare = vc[vc < 0.01]\n",
    "rare_labels = rare.index.tolist()\n",
    "df['Country_grouped'] = df['Country'].replace(rare_labels, 'Other')\n",
    "\n",
    "!pip install matplotlib-venn\n",
    "\n",
    "# https://pypi.python.org/pypi/libarchive\n",
    "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
    "import libarchive\n",
    "\n",
    "# https://pypi.python.org/pypi/pydot\n",
    "!apt-get -qq install -y graphviz && pip install pydot\n",
    "import pydot\n",
    "\n",
    "!pip install cartopy\n",
    "import cartopy\n",
    "\n",
    "!apt-get update -qq\n",
    "!apt-get install -y libarchive-dev\n",
    "\n",
    "!pip install libarchive-c\n",
    "\n",
    "# Test\n",
    "import libarchive\n",
    "print(\"Import succeeded:\", libarchive)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "df['treatment'] = df['treatment'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 2. Define features and target\n",
    "cat_cols = [\n",
    "    'Gender', 'Country_grouped', 'self_employed', 'family_history',\n",
    "    'work_interfere', 'remote_work', 'tech_company', 'benefits',\n",
    "    'care_options', 'wellness_program', 'seek_help', 'leave',\n",
    "    'mental_health_consequence', 'phys_health_consequence',\n",
    "    'coworkers', 'supervisor', 'mental_health_interview',\n",
    "    'phys_health_interview', 'mental_vs_physical', 'obs_consequence'\n",
    "]\n",
    "num_cols = ['Age']\n",
    "target = 'treatment'\n",
    "\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0C-9pIbwYE8"
   },
   "outputs": [],
   "source": [
    "# 3. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ohsy34Ol0GLP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0PKQXHp0bZQ"
   },
   "outputs": [],
   "source": [
    "def apply_pseudo_labeling(base_model, X_labeled, y_labeled, X_unlabeled, preprocessor=None, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Apply Pseudo-Labeling for models that require preprocessed numerical features\n",
    "    (e.g., Logistic Regression, Random Forest, XGBoost).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_model: sklearn-style model (must have fit() and predict_proba())\n",
    "    X_labeled: pd.DataFrame\n",
    "        Features of the labeled data\n",
    "    y_labeled: pd.Series or np.array\n",
    "        Labels of the labeled data\n",
    "    X_unlabeled: pd.DataFrame\n",
    "        Features of the unlabeled data\n",
    "    preprocessor: ColumnTransformer or None\n",
    "        Preprocessing pipeline to apply. Must be fitted inside this function.\n",
    "    threshold: float (default=0.9)\n",
    "        Confidence threshold for selecting pseudo-labeled samples.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_augmented: pd.DataFrame\n",
    "        Augmented training features (labeled + high-confidence pseudo-labeled)\n",
    "    y_augmented: pd.Series\n",
    "        Augmented training labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit and transform preprocessing if provided\n",
    "    if preprocessor is not None:\n",
    "        preprocessor.fit(X_labeled)\n",
    "        X_labeled_encoded = preprocessor.transform(X_labeled)\n",
    "        X_unlabeled_encoded = preprocessor.transform(X_unlabeled)\n",
    "    else:\n",
    "        X_labeled_encoded = X_labeled\n",
    "        X_unlabeled_encoded = X_unlabeled\n",
    "\n",
    "    # Train the base model\n",
    "    base_model.fit(X_labeled_encoded, y_labeled)\n",
    "\n",
    "    # Predict pseudo-labels for unlabeled data\n",
    "    proba_unlabeled = base_model.predict_proba(X_unlabeled_encoded)\n",
    "    preds_unlabeled = np.argmax(proba_unlabeled, axis=1)\n",
    "    max_proba = np.max(proba_unlabeled, axis=1)\n",
    "\n",
    "    # Select high-confidence pseudo-labeled samples\n",
    "    mask = max_proba >= threshold\n",
    "    X_pseudo = X_unlabeled.iloc[mask]\n",
    "    y_pseudo = preds_unlabeled[mask]\n",
    "\n",
    "    # Combine original labeled data with pseudo-labeled data\n",
    "    X_augmented = pd.concat([X_labeled, X_pseudo], axis=0)\n",
    "    y_augmented = pd.concat([y_labeled, pd.Series(y_pseudo, index=X_pseudo.index)], axis=0)\n",
    "\n",
    "    return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRl-7GVO0c52",
    "outputId": "55bed09a-833a-4469-d267-6bfdfb6c7571"
   },
   "outputs": [],
   "source": [
    "# Fill missing values for categorical columns\n",
    "X[cat_cols] = X[cat_cols].fillna('Missing').astype(str)\n",
    "\n",
    "# Preprocessor for LR, RF, XGB\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [('onehot', ohe, cat_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Base models\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Pipelines for models needing preprocessing\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('classifier', xgb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWzOKOck0nB2"
   },
   "outputs": [],
   "source": [
    "# Further split X_train_base into Labeled and Unlabeled simulation\n",
    "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(\n",
    "    X_train, y_train, stratify=y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDHQIhwl0ed9",
    "outputId": "2f3716cd-2238-4a36-bd17-b6ad3d503ff1"
   },
   "outputs": [],
   "source": [
    "print(\"===== PSEUDO-LABELING =====\")\n",
    "\n",
    "# XGBoost (Pseudo-Labeling)\n",
    "X_aug_xgb, y_aug_xgb = apply_pseudo_labeling(xgb, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor, threshold=0.95)\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_aug_xgb, y_aug_xgb, stratify=y_aug_xgb, test_size=0.2, random_state=42)\n",
    "xgb_pipeline.fit(X_train_xgb, y_train_xgb)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test_xgb)\n",
    "print(\"Pseudo-Labeling XGBoost:\\n\", classification_report(y_test_xgb, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3zeiztB0gj6",
    "outputId": "60a78a2e-b000-4f98-db96-2633c7575e22"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"===== FINE-TUNING PSEUDO-LABELING XGBOOST (Small LR + More Trees) =====\")\n",
    "\n",
    "# 1. Pseudo-labeling\n",
    "X_aug_xgb, y_aug_xgb = apply_pseudo_labeling(xgb, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor, threshold=0.95)\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n",
    "    X_aug_xgb, y_aug_xgb, stratify=y_aug_xgb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Column types\n",
    "categorical_cols = X_train_xgb.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train_xgb.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# 4. Preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ('num', StandardScaler(), numerical_cols)\n",
    "])\n",
    "\n",
    "# 5. Pipeline\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# 6. New hyperparameter search space\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [300, 400, 500, 600, 800],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__gamma': [0, 0.05, 0.1],\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1],\n",
    "    'classifier__reg_lambda': [1, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# 7. RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 8. Fit\n",
    "random_search.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# 9. Predict\n",
    "best_xgb_pipeline = random_search.best_estimator_\n",
    "y_pred_xgb = best_xgb_pipeline.predict(X_test_xgb)\n",
    "\n",
    "# 10. Report\n",
    "print(\"\\n===== Best Parameters =====\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "print(\"\\n===== Best Cross-Validation Accuracy =====\")\n",
    "print(random_search.best_score_)\n",
    "\n",
    "print(\"\\n===== Test Set Classification Report =====\")\n",
    "print(classification_report(y_test_xgb, y_pred_xgb))\n",
    "\n",
    "# 11. Save model and best params\n",
    "save_dir = \"/content/models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(best_xgb_pipeline, os.path.join(save_dir, \"best_xgb_pipeline_v2.pkl\"))\n",
    "print(f\"Model saved to {save_dir}/best_xgb_pipeline_v2.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, \"best_xgb_params_v2.json\"), \"w\") as f:\n",
    "    json.dump(random_search.best_params_, f, indent=4)\n",
    "print(f\"Best parameters saved to {save_dir}/best_xgb_params_v2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujgB_bo03wqf"
   },
   "source": [
    "## Fariness Analysis Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p4sA2-y35cl"
   },
   "source": [
    "### **Sensitive Feature Extraction**\n",
    "\n",
    "In fairness evaluation, sensitive features are chosen based on their relevance to societal bias, historical disparities, and the potential for unequal treatment outcomes.  \n",
    "For this mental health treatment prediction task, we selected **Gender, Age Group, Self-Employment Status, and Family History** as sensitive features because they are closely linked to historical disparities in mental health access and diagnosis, and may influence the likelihood of being identified for treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GslD7P790r8c"
   },
   "outputs": [],
   "source": [
    "# === 1. Process Age: Binning into age_group ===\n",
    "import pandas as pd\n",
    "\n",
    "# Assume your X_test already contains a column named \"Age\"\n",
    "# Define binning rules, for example: 18-25, 26-35, 36-45, 45+\n",
    "bins = [0, 25, 35, 45, 100]\n",
    "labels = ['18-25', '26-35', '36-45', '45+']\n",
    "X_test[\"age_group\"] = pd.cut(X_test[\"Age\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# === 2. Process Gender: Cleaning and standardization ===\n",
    "\n",
    "# Standardize the \"Gender\" field into 'male' / 'female' / 'other'\n",
    "def clean_gender(gender):\n",
    "    if pd.isnull(gender):\n",
    "        return \"Unknown\"\n",
    "    gender = gender.strip().lower()\n",
    "    if gender in [\"male\", \"m\", \"man\", \"cis male\", \"cis man\"]:\n",
    "        return \"male\"\n",
    "    elif gender in [\"female\", \"f\", \"woman\", \"cis female\", \"cis woman\"]:\n",
    "        return \"female\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "X_test[\"gender_clean\"] = X_test[\"Gender\"].apply(clean_gender)\n",
    "\n",
    "# (Optional) You may merge \"other\" and \"Unknown\" into a single group\n",
    "# depending on the sample size of each group\n",
    "\n",
    "# === 3. Create the sensitive_features dataframe ===\n",
    "sensitive_features = X_test[[\"gender_clean\", \"age_group\"]].copy()\n",
    "sensitive_features.columns = [\"gender\", \"age_group\"]  # Rename for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nmrtIuz4S_g",
    "outputId": "c90ba7cc-4a89-45c1-b042-0b966bb00930"
   },
   "outputs": [],
   "source": [
    "!pip install fairlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7LH41k-mvuM"
   },
   "source": [
    "### Accuracy and Selection Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJC2Ww9p2BWS",
    "outputId": "5b95d8ec-da0e-4e0a-eb6d-6464169b3e89"
   },
   "outputs": [],
   "source": [
    "# === Import necessary packages for fairness analysis ===\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_ratio, equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score as sk_accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = best_xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Fairness analysis by Gender\n",
    "print(\"=== Gender Fairness Analysis ===\")\n",
    "gender_metric = MetricFrame(\n",
    "    metrics={\"accuracy\": sk_accuracy_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_xgb,\n",
    "    sensitive_features=sensitive_features[\"gender\"]\n",
    ")\n",
    "print(gender_metric.by_group)\n",
    "\n",
    "# Fairness analysis by Age Group\n",
    "print(\"\\n=== Age Group Fairness Analysis ===\")\n",
    "age_metric = MetricFrame(\n",
    "    metrics={\"accuracy\": sk_accuracy_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_xgb,\n",
    "    sensitive_features=sensitive_features[\"age_group\"]\n",
    ")\n",
    "print(age_metric.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8ZNmOCp5feA",
    "outputId": "08ecd769-192f-43cd-d6fc-eba2f02505f2"
   },
   "outputs": [],
   "source": [
    "# === 1. Import necessary packages ===\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "from sklearn.metrics import accuracy_score as sk_accuracy_score\n",
    "\n",
    "# === 2. Make prediction ===\n",
    "y_pred_xgb = best_xgb_pipeline.predict(X_test)\n",
    "\n",
    "# === 3. Map logical feature name to real column name ===\n",
    "feature_mapping = {\n",
    "    \"Gender\": \"gender_clean\",\n",
    "    \"Age Group\": \"age_group\",\n",
    "    \"Self-Employed\": \"self_employed\",\n",
    "    \"Country\": \"Country_grouped\",\n",
    "    \"Family History\": \"family_history\"\n",
    "}\n",
    "\n",
    "# === 4. For each sensitive feature, do fairness analysis and print table ===\n",
    "for logical_name, real_column in feature_mapping.items():\n",
    "    print(f\"\\n=== Fairness Analysis by {logical_name} ===\")\n",
    "\n",
    "    # Create MetricFrame for fairness metrics\n",
    "    metric = MetricFrame(\n",
    "        metrics={\"accuracy\": sk_accuracy_score, \"selection_rate\": selection_rate},\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_xgb,\n",
    "        sensitive_features=X_test[real_column]\n",
    "    )\n",
    "\n",
    "    # Calculate sample size per group\n",
    "    sample_counts = X_test[real_column].value_counts().to_dict()\n",
    "\n",
    "    # Print nicely\n",
    "    result_df = metric.by_group.copy()\n",
    "    result_df[\"n_samples\"] = result_df.index.map(lambda x: sample_counts.get(x, 0))\n",
    "\n",
    "    # Reorder columns\n",
    "    result_df = result_df[[\"n_samples\", \"accuracy\", \"selection_rate\"]]\n",
    "\n",
    "    print(result_df)\n",
    "\n",
    "    # Warning for small groups\n",
    "    for group, n in sample_counts.items():\n",
    "        if n < 10:\n",
    "            print(f\"\\nWarning: '{group}' has only {n} samples. Interpret fairness metrics with caution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1L9aQYXHpOy"
   },
   "source": [
    "## **Fairness Analysis (Accuracy and Selection Rate)**\n",
    "This fairness analysis evaluates the model’s prediction performance across different sensitive groups, based on the mental health treatment survey dataset. The objective is to identify potential biases or disparities in prediction outcomes across demographic and employment-related features.\n",
    "\n",
    "The analysis focuses on four main dimensions: **Gender**, **Age Group**, **Self-Employment Status**, and **Family History**. Although **Country** information is available, due to severe sample imbalance across countries, country-level analysis was not included in the main findings.\n",
    "\n",
    "\n",
    "## **1. Gender Fairness**\n",
    "\n",
    "| Gender | n_samples | Accuracy | Selection Rate |\n",
    "|:---|:---|:---|:---|\n",
    "| Female | 48 | 91.7% | 77.1% |\n",
    "| Male | 202 | 80.2% | 49.1% |\n",
    "| Other | 1 | 100.0% | 100.0% |\n",
    "\n",
    "**Findings**:\n",
    "- The model predicts treatment needs for **female** respondents with significantly higher accuracy (91.7%) compared to **male** respondents (80.2%).\n",
    "- **Female** respondents are more likely to be predicted as needing treatment, with a selection rate of 77.1% versus 49.1% for males.\n",
    "- The \"other\" gender group contains only **one** sample, and therefore results for this group are not statistically reliable.\n",
    "\n",
    "**Insight**:\n",
    "- There is a noticeable **underprediction risk for male respondents**, suggesting the model may be less sensitive to treatment needs among males.\n",
    "- Future improvement efforts could focus on boosting recall or balancing sensitivity across genders.\n",
    "\n",
    "## **2. Age Group Fairness**\n",
    "\n",
    "| Age Group | n_samples | Accuracy | Selection Rate |\n",
    "|:---|:---|:---|:---|\n",
    "| 18–25 | 31 | 80.6% | 41.9% |\n",
    "| 26–35 | 140 | 85.7% | 51.4% |\n",
    "| 36–45 | 63 | 76.2% | 66.7% |\n",
    "| 45+ | 17 | 82.4% | 58.8% |\n",
    "\n",
    "**Findings**:\n",
    "- The model performs best for the **26–35** age group, achieving 85.7% accuracy and a moderate selection rate.\n",
    "- The **36–45** group shows the lowest accuracy (76.2%) despite a relatively high selection rate (66.7%).\n",
    "- The **18–25** group has a notably low selection rate (41.9%), suggesting a risk of **missing younger individuals** who might need treatment.\n",
    "\n",
    "**Insight**:\n",
    "- **Potential underprediction exists among the youngest group (18–25 years old)**, which could lead to untreated mental health issues if not addressed.\n",
    "- The model could benefit from calibration to ensure fair sensitivity across age groups, especially for younger respondents.\n",
    "\n",
    "## **3. Self-Employment Status Fairness**\n",
    "\n",
    "| Self-Employment | n_samples | Accuracy | Selection Rate |\n",
    "|:---|:---|:---|:---|\n",
    "| No | 224 | 83.9% | 52.2% |\n",
    "| Yes | 27 | 70.4% | 74.1% |\n",
    "\n",
    "**Findings**:\n",
    "- The model shows a noticeable performance gap: self-employed individuals have a lower accuracy (70.4%) compared to non-self-employed individuals (83.9%).\n",
    "- Self-employed respondents have a **higher selection rate** (74.1%), suggesting they are more likely to be predicted as needing treatment.\n",
    "\n",
    "**Insight**:\n",
    "- There is a risk of **overprediction** for the self-employed group, which could lead to false positives.\n",
    "- Special attention may be needed to better capture the mental health profiles of self-employed individuals without overflagging them.\n",
    "\n",
    "## **4. Family History Fairness**\n",
    "\n",
    "| Family History | n_samples | Accuracy | Selection Rate |\n",
    "|:---|:---|:---|:---|\n",
    "| No | 150 | 77.3% | 36.7% |\n",
    "| Yes | 101 | 90.1% | 81.2% |\n",
    "\n",
    "**Findings**:\n",
    "- Respondents with a family history of mental illness are predicted with much higher accuracy (90.1%) compared to those without (77.3%).\n",
    "- The selection rate for individuals without family history is substantially lower (36.7%).\n",
    "\n",
    "**Insight**:\n",
    "- **Underprediction risk is significant for individuals without a family history**, indicating potential gaps in detecting treatment needs among this group.\n",
    "- Model recalibration or feature engineering could help bridge this gap.\n",
    "\n",
    "\n",
    "## **5. Country Analysis**\n",
    "\n",
    "Although country information was available, **the distribution across countries was highly imbalanced**, with the United States and United Kingdom comprising the majority of samples (152 and 38 respectively), while other countries had very small sample sizes (4–9 samples each).\n",
    "\n",
    "| Country | n_samples |\n",
    "|:---|:---|\n",
    "| United States | 152 |\n",
    "| United Kingdom | 38 |\n",
    "| Other countries (Australia, Germany, Ireland, Netherlands, etc.) | 4–9 |\n",
    "\n",
    "**Reasoning for Exclusion**:\n",
    "- Small sample sizes in countries such as Netherlands, Germany, Ireland, and Australia lead to statistically unstable accuracy and selection rate estimates.\n",
    "- In small groups, even a single misclassification can significantly skew performance metrics (e.g., a 10%+ drop in accuracy).\n",
    "- Including country-level fairness analysis could introduce noise without yielding reliable business insights.\n",
    "\n",
    "**Professional Statement**:\n",
    "> \"Although country information was available, we observed a highly imbalanced distribution across countries, with the United States accounting for a majority of samples. Due to the small sample sizes in other countries, we excluded country-level fairness analysis from the main findings to avoid unreliable conclusions based on statistical noise.\"\n",
    "\n",
    "Thus, **country was not considered a primary fairness dimension** in the final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOd-L_rkmrDC"
   },
   "source": [
    "### Disparate Impact and Equalized Odds Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwQHPeMVRI4L",
    "outputId": "afca37f7-fc6a-455e-c6e8-9222afc3a173"
   },
   "outputs": [],
   "source": [
    "# === Import necessary packages ===\n",
    "from fairlearn.metrics import (\n",
    "    selection_rate,\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference,\n",
    "    MetricFrame\n",
    ")\n",
    "\n",
    "# === Make prediction if not yet ===\n",
    "# y_pred_xgb = best_xgb_pipeline.predict(X_test)\n",
    "\n",
    "# === Map logical names to real column names\n",
    "feature_mapping = {\n",
    "    \"Gender\": \"gender_clean\",\n",
    "    \"Age Group\": \"age_group\",\n",
    "    \"Self-Employed\": \"self_employed\",\n",
    "    \"Family History\": \"family_history\"\n",
    "}\n",
    "\n",
    "# === Only output DI and EOD for each sensitive feature ===\n",
    "for logical_name, real_column in feature_mapping.items():\n",
    "    print(f\"\\n=== Fairness Summary for {logical_name} ===\")\n",
    "\n",
    "    # 1. Disparate Impact\n",
    "    selection_rates = MetricFrame(\n",
    "        metrics=selection_rate,\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_xgb,\n",
    "        sensitive_features=X_test[real_column]\n",
    "    ).by_group\n",
    "    di = selection_rates.min() / selection_rates.max()\n",
    "\n",
    "    # 2. Equalized Odds Difference\n",
    "    eod = equalized_odds_difference(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_xgb,\n",
    "        sensitive_features=X_test[real_column]\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Disparate Impact (DI): {di:.3f} (Ideal: 1)\")\n",
    "    print(f\"Equalized Odds Difference (EOD): {eod:.3f} (Ideal: 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlZd0XO1Tp7W"
   },
   "source": [
    "# **Fairness Analysis (Disparate Impact and Equalized Odds Difference)**\n",
    "\n",
    "The fairness evaluation focuses on four sensitive attributes: **Gender**, **Age Group**, **Self-Employment Status**, and **Family History** of mental health conditions.\n",
    "\n",
    "Two key fairness metrics were assessed:\n",
    "- **Disparate Impact (DI)**: Measures the ratio of positive prediction rates between groups. A value close to 1 indicates fairness.\n",
    "- **Equalized Odds Difference (EOD)**: Measures the difference in True Positive Rate (TPR) and False Positive Rate (FPR) across groups. A value close to 0 indicates fairness.\n",
    "\n",
    "### Fairness Analysis by Sensitive Feature\n",
    "\n",
    "### Gender\n",
    "\n",
    "- **Disparate Impact (DI)**: 0.490 (Ideal: 1)\n",
    "- **Equalized Odds Difference (EOD)**: 0.221 (Ideal: 0)\n",
    "\n",
    "**Findings**:\n",
    "- Males are significantly less likely to be predicted as needing treatment compared to females.\n",
    "- There is a substantial difference in the True Positive and False Positive Rates between male and female groups.\n",
    "\n",
    "**Potential Risks**:\n",
    "- Male individuals may be under-detected by the model, leading to potential delays or misses in necessary mental health interventions.\n",
    "- Gender bias could result in unequal access to treatment recommendations.\n",
    "\n",
    "### Age Group\n",
    "\n",
    "- **Disparate Impact (DI)**: 0.629 (Ideal: 1)\n",
    "- **Equalized Odds Difference (EOD)**: 0.385 (Ideal: 0)\n",
    "\n",
    "**Findings**:\n",
    "- Younger individuals, particularly the 18–25 age group, have lower positive prediction rates compared to older groups.\n",
    "- A large disparity in TPR and FPR is observed among different age groups.\n",
    "\n",
    "**Potential Risks**:\n",
    "- Younger populations might experience higher false negative rates, leading to underdiagnosis.\n",
    "- Age-related fairness issues could skew treatment accessibility for younger individuals.\n",
    "\n",
    "### Self-Employed Status\n",
    "\n",
    "- **Disparate Impact (DI)**: 0.705 (Ideal: 1)\n",
    "- **Equalized Odds Difference (EOD)**: 0.358 (Ideal: 0)\n",
    "\n",
    "**Findings**:\n",
    "- Self-employed individuals are more likely to be predicted as needing treatment compared to non-self-employed individuals.\n",
    "- A considerable difference exists in prediction accuracy between these two groups.\n",
    "\n",
    "**Potential Risks**:\n",
    "- Overprediction among self-employed individuals could lead to unnecessary interventions.\n",
    "- Unequal treatment recommendations could impact trust and perceived model credibility among self-employed populations.\n",
    "\n",
    "### Family History\n",
    "\n",
    "- **Disparate Impact (DI)**: 0.452 (Ideal: 1)\n",
    "- **Equalized Odds Difference (EOD)**: 0.268 (Ideal: 0)\n",
    "\n",
    "**Findings**:\n",
    "- Individuals without a family history of mental illness are significantly less likely to be predicted as needing treatment.\n",
    "- Prediction performance differs substantially between groups with and without family history.\n",
    "\n",
    "**Potential Risks**:\n",
    "- Individuals without family history might be overlooked despite having actual treatment needs.\n",
    "- Family history bias could reinforce inaccurate assumptions about mental health risks.\n",
    "\n",
    "## Overall Conclusion\n",
    "\n",
    "The fairness evaluation reveals significant concerns across multiple sensitive dimensions:\n",
    "\n",
    "- **Gender** and **Family History** demonstrate the most pronounced disparities, with Disparate Impact values far below 0.8 and notable Equalized Odds Differences.\n",
    "- **Age Group** and **Self-Employment Status** also show meaningful fairness issues that warrant further investigation.\n",
    "- Without mitigation strategies, these disparities could lead to inequitable treatment recommendations, reinforcing existing barriers to mental health access for vulnerable groups.\n",
    "\n",
    "**Recommendations**:\n",
    "- Prioritize fairness improvements for Gender and Family History attributes.\n",
    "- Consider bias mitigation techniques such as reweighting, fairness-constrained optimization, or targeted model calibration.\n",
    "- Regularly monitor fairness metrics post-deployment to ensure continuous compliance with ethical and operational standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vJ7EVcVAsp7"
   },
   "source": [
    "## **FairML for Bias Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIda5U-6Avwk"
   },
   "source": [
    "We initially planned to use FairML to audit feature bias, but Since we used Google Colab, FairML can not be pip installed.\n",
    "\n",
    "Instead, we switched to Fairlearn, which is fully supported and works well with modern systems. Using **Fairlearn's Exponentiated Gradient** method, we were still able to measure how much each feature contributes to potential bias — and the results are interpretable and robust.\n",
    "\n",
    "So this way, we ensured both technical stability and meaningful insights into our model's fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpmPfbfs74KM"
   },
   "outputs": [],
   "source": [
    "# === 1. Import\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# === 2. Encode all non-numerical columns (object and category types) ===\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in X_test_encoded.columns:\n",
    "    if X_test_encoded[col].dtype == \"object\" or str(X_test_encoded[col].dtype).startswith(\"category\"):\n",
    "        le = LabelEncoder()\n",
    "        X_test_encoded[col] = le.fit_transform(X_test_encoded[col].astype(str))\n",
    "\n",
    "# === 3. Encode sensitive feature separately\n",
    "sensitive_feature_name = \"gender_clean\"  # or \"age_group\" / \"self_employed\" / \"family_history\"\n",
    "sensitive_feature_encoded = LabelEncoder().fit_transform(X_test[sensitive_feature_name].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWmZagEtAET1",
    "outputId": "de6c93f1-0cf4-4326-d0f5-13dcd1d0d294"
   },
   "outputs": [],
   "source": [
    "# === 4. Fit fairness-aware model\n",
    "constraint = DemographicParity()\n",
    "base_model = LogisticRegression(solver=\"liblinear\")\n",
    "exp_grad = ExponentiatedGradient(base_model, constraints=constraint)\n",
    "\n",
    "exp_grad.fit(X_test_encoded, y_test, sensitive_features=sensitive_feature_encoded)\n",
    "\n",
    "# === 5. Get feature bias contribution\n",
    "# Collect all base classifiers and their weights\n",
    "coefs = []\n",
    "weights = []\n",
    "\n",
    "for classifier, weight in zip(exp_grad.predictors_, exp_grad.weights_):\n",
    "    coefs.append(classifier.coef_[0])  # assume binary classification\n",
    "    weights.append(weight)\n",
    "\n",
    "# Weighted average of coefficients\n",
    "import numpy as np\n",
    "\n",
    "weighted_coef = np.average(coefs, axis=0, weights=weights)\n",
    "\n",
    "# Turn into Series for readability\n",
    "feature_bias = pd.Series(weighted_coef, index=X_test_encoded.columns)\n",
    "feature_bias = feature_bias.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n=== Global Feature Importance for Bias (Demographic Parity) ===\")\n",
    "print(feature_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qY1e-WTkB5wS",
    "outputId": "bfdef661-8600-4411-d23a-69d3b6cfbfe4"
   },
   "outputs": [],
   "source": [
    "# === Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Sort feature_bias ascending\n",
    "feature_bias_sorted = feature_bias.sort_values(ascending=True)\n",
    "\n",
    "# === Prepare color gradient (light blue to deep blue)\n",
    "cmap = plt.get_cmap('Blues')\n",
    "colors = cmap(np.linspace(0.4, 0.9, len(feature_bias_sorted)))  # 0.4 to 0.9 controls brightness\n",
    "\n",
    "# === Plot\n",
    "plt.figure(figsize=(8, 10))\n",
    "bars = plt.barh(feature_bias_sorted.index, feature_bias_sorted.values, color=colors)\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Bias Contribution (absolute coefficient)\")\n",
    "plt.title(\"Global Feature Importance for Bias (Demographic Parity)\")\n",
    "\n",
    "# Add grid\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Make layout tight\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5UdfqbpE_fP"
   },
   "source": [
    "## Bias Analysis Analysis\n",
    "\n",
    "This section evaluates the fairness of the mental health treatment prediction model by analyzing the contribution of various features to bias under the **Demographic Parity** constraint.  \n",
    "The goal is to identify key drivers of unfairness and provide recommendations for future model enhancements.\n",
    "\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on a global feature importance audit:\n",
    "\n",
    "- **Age Group** is the largest contributor to model bias, with an absolute coefficient significantly higher than other features.  \n",
    "  Different age groups are treated differently by the model, suggesting a risk of **age-related unfairness**.\n",
    "\n",
    "- **Work Interference** and **Family History** are the second and third most significant drivers.  \n",
    "  The model heavily relies on whether mental health interferes with work and whether individuals have a family history of mental illness, which might reinforce existing societal inequalities.\n",
    "\n",
    "- **Gender** shows a noticeable bias contribution (~0.29), highlighting the need to monitor potential **gender disparities** in treatment predictions.\n",
    "\n",
    "- **Employment Type**, specifically **Self-Employed** status and **Remote Work** arrangements, also introduce moderate bias, indicating that non-traditional employment groups might be unfairly treated.\n",
    "\n",
    "- **Country Group** and **Mental vs Physical Health Perception** have negligible bias contributions, suggesting that geographical and broad health conceptual biases are not major concerns for this model.\n",
    "\n",
    "### Further Enhancements\n",
    "\n",
    "To improve fairness and mitigate identified risks, we propose the following strategies:\n",
    "\n",
    "- **Group Reweighting**:  \n",
    "  Apply instance weighting or resampling techniques to balance underrepresented sensitive groups (e.g., younger age groups, self-employed individuals) during model training.\n",
    "\n",
    "- **Fairness-Constrained Model Optimization**:  \n",
    "  Incorporate fairness constraints (e.g., Demographic Parity, Equalized Odds) directly into the model training process using methods such as `ExponentiatedGradient` or adversarial debiasing.\n",
    "\n",
    "- **Feature Auditing and Debiasing**:  \n",
    "  Consider removing or down-weighting sensitive features (e.g., family history) or applying feature perturbation analysis to detect overreliance before finalizing the model.\n",
    "\n",
    "- **Post-Deployment Monitoring**:  \n",
    "  Continuously monitor the model’s fairness after deployment using real-world data, especially for age and gender groups, to detect and respond to any bias drift over time.\n",
    "\n",
    "- **Transparency and Documentation**:  \n",
    "  Clearly document the known bias risks and mitigation strategies in model cards or transparency reports shared with stakeholders.\n",
    "\n",
    "### Summary\n",
    "\n",
    "> While the model performs well overall, fairness audits reveal bias risks associated with age, work interference, family background, and gender. Targeted enhancements, including group reweighting, fairness-constrained optimization, and post-deployment monitoring, are recommended to ensure more equitable outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SPZNid4DJPi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
