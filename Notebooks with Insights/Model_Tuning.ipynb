{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Oxgj73CYqcbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "F2R_rCFI2iAP",
        "outputId": "ea92efc1-b3a8-4051-89b8-cf6cf322dfd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Timestamp  Age  Gender         Country state self_employed  \\\n",
              "0  2014-08-27 11:29:31   37  Female   United States    IL           NaN   \n",
              "1  2014-08-27 11:29:37   44       M   United States    IN           NaN   \n",
              "2  2014-08-27 11:29:44   32    Male          Canada   NaN           NaN   \n",
              "3  2014-08-27 11:29:46   31    Male  United Kingdom   NaN           NaN   \n",
              "4  2014-08-27 11:30:22   31    Male   United States    TX           NaN   \n",
              "\n",
              "  family_history treatment work_interfere    no_employees  ...  \\\n",
              "0             No       Yes          Often            6-25  ...   \n",
              "1             No        No         Rarely  More than 1000  ...   \n",
              "2             No        No         Rarely            6-25  ...   \n",
              "3            Yes       Yes          Often          26-100  ...   \n",
              "4             No        No          Never         100-500  ...   \n",
              "\n",
              "                leave mental_health_consequence phys_health_consequence  \\\n",
              "0       Somewhat easy                        No                      No   \n",
              "1          Don't know                     Maybe                      No   \n",
              "2  Somewhat difficult                        No                      No   \n",
              "3  Somewhat difficult                       Yes                     Yes   \n",
              "4          Don't know                        No                      No   \n",
              "\n",
              "      coworkers supervisor mental_health_interview phys_health_interview  \\\n",
              "0  Some of them        Yes                      No                 Maybe   \n",
              "1            No         No                      No                    No   \n",
              "2           Yes        Yes                     Yes                   Yes   \n",
              "3  Some of them         No                   Maybe                 Maybe   \n",
              "4  Some of them        Yes                     Yes                   Yes   \n",
              "\n",
              "  mental_vs_physical obs_consequence comments  \n",
              "0                Yes              No      NaN  \n",
              "1         Don't know              No      NaN  \n",
              "2                 No              No      NaN  \n",
              "3                 No             Yes      NaN  \n",
              "4         Don't know              No      NaN  \n",
              "\n",
              "[5 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9d91d16-e00f-48a6-989a-6fbe77c8f091\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Country</th>\n",
              "      <th>state</th>\n",
              "      <th>self_employed</th>\n",
              "      <th>family_history</th>\n",
              "      <th>treatment</th>\n",
              "      <th>work_interfere</th>\n",
              "      <th>no_employees</th>\n",
              "      <th>...</th>\n",
              "      <th>leave</th>\n",
              "      <th>mental_health_consequence</th>\n",
              "      <th>phys_health_consequence</th>\n",
              "      <th>coworkers</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>mental_health_interview</th>\n",
              "      <th>phys_health_interview</th>\n",
              "      <th>mental_vs_physical</th>\n",
              "      <th>obs_consequence</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-08-27 11:29:31</td>\n",
              "      <td>37</td>\n",
              "      <td>Female</td>\n",
              "      <td>United States</td>\n",
              "      <td>IL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Often</td>\n",
              "      <td>6-25</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat easy</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-08-27 11:29:37</td>\n",
              "      <td>44</td>\n",
              "      <td>M</td>\n",
              "      <td>United States</td>\n",
              "      <td>IN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Rarely</td>\n",
              "      <td>More than 1000</td>\n",
              "      <td>...</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-08-27 11:29:44</td>\n",
              "      <td>32</td>\n",
              "      <td>Male</td>\n",
              "      <td>Canada</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Rarely</td>\n",
              "      <td>6-25</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat difficult</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014-08-27 11:29:46</td>\n",
              "      <td>31</td>\n",
              "      <td>Male</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Often</td>\n",
              "      <td>26-100</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat difficult</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>No</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-08-27 11:30:22</td>\n",
              "      <td>31</td>\n",
              "      <td>Male</td>\n",
              "      <td>United States</td>\n",
              "      <td>TX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Never</td>\n",
              "      <td>100-500</td>\n",
              "      <td>...</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9d91d16-e00f-48a6-989a-6fbe77c8f091')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9d91d16-e00f-48a6-989a-6fbe77c8f091 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9d91d16-e00f-48a6-989a-6fbe77c8f091');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f8623a3e-0b3f-4d51-a056-d994843806e4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8623a3e-0b3f-4d51-a056-d994843806e4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f8623a3e-0b3f-4d51-a056-d994843806e4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/survey.csv'\n",
        "df = pd.read_csv(path, encoding='latin1')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['comments', 'Timestamp', 'state', 'no_employees', 'anonymity'])\n",
        "df = df[(df['Age'] >= 18) & (df['Age'] <= 100)]\n",
        "df['self_employed'] = df['self_employed'].fillna('No')\n",
        "df.drop_duplicates()\n",
        "df['work_interfere'] = df['work_interfere'].fillna('Not applicable')"
      ],
      "metadata": {
        "id": "BFrGer5W442N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to clean the gender variable into 'female', 'male', and 'other'\n",
        "def clean_gender(gender):\n",
        "    gender = str(gender).strip().lower()\n",
        "    if gender in ['male', 'm', 'cis male', 'male (cis)', 'man', 'mail', 'cis man', 'malr', 'make', 'maile']:\n",
        "        return 'Male'\n",
        "    elif gender in ['female', 'f', 'cis female', 'woman', 'female (cis)', 'cis-female/femme', 'femake', 'femail', 'female ', 'trans female']:\n",
        "        return 'Female'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "# Apply the cleaning function\n",
        "df['Gender'] = df['Gender'].apply(clean_gender)\n",
        "\n",
        "#verify the results\n",
        "print(df['Gender'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Zsnn6M6LeW",
        "outputId": "ff91c9da-9a8a-4de5-fc88-1670c993f944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gender\n",
            "Male      983\n",
            "Female    247\n",
            "Other      21\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"treatment\"\n",
        "print(df[target].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFrQsCQc6MyN",
        "outputId": "1fe30c36-2086-429c-93ee-2f42ac9ea925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "treatment\n",
            "Yes    0.505196\n",
            "No     0.494804\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "hg1nR7nbWN1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embeddings"
      ],
      "metadata": {
        "id": "69VA0YLlBkBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why We Use Embedding\n",
        "\n",
        "In this mental health treatment prediction task, one of the key challenges was dealing with **categorical variables**, especially those with a **high number of unique values** (high cardinality). Traditional preprocessing methods like **One-Hot Encoding** or **Ordinal Encoding** are inefficient or ineffective for such variables because:\n",
        "\n",
        "* One-Hot Encoding leads to **high-dimensional sparse vectors**, which are computationally expensive and prone to overfitting.\n",
        "* Ordinal Encoding imposes a **false sense of order** among categories, which doesn’t reflect real-world meaning.\n",
        "\n",
        "To address this, we applied **categorical embeddings**, which map categories to dense, low-dimensional vectors learned based on **task relevance**. These embeddings capture **semantic similarity** between categories and are more scalable.\n",
        "\n",
        "### What Was Embedded\n",
        "\n",
        "We first screened all categorical variables based on **cardinality** (133 number of unique values), thus only `country` qualified as **high-cardinality** and was selected for **embedding**. Other categorical variables were handled through standard encoding.\n",
        "\n",
        "### How to Do\n",
        "\n",
        "We implemented **learned embeddings** for the `country` variable using the following steps:\n",
        "\n",
        "#### Step 1: Preprocess Input\n",
        "\n",
        "* `country` entries were cleaned (e.g., lowercased, whitespace removed).\n",
        "* Rare countries with extremely low frequency were grouped as `\"Other\"` to stabilize the embedding.\n",
        "\n",
        "#### Step 2: Integer Encoding\n",
        "\n",
        "* Mapped each unique country to an integer ID using `LabelEncoder`.\n",
        "\n",
        "#### Step 3: Learnable Embedding\n",
        "\n",
        "* We created an embedding layer to transform the high-cardinality categorical variable Country into a low-dimensional, dense vector representation.\n",
        "Instead of using one-hot encoding, which creates sparse and uninformative vectors, the embedding layer learns 8-dimensional trainable embeddings for each unique country.\n",
        "* The embedding layer was trained jointly with the rest of the model.\n",
        "\n",
        "#### Step 4: Export Embedding\n",
        "\n",
        "* After training, extracted the embedding vectors for each country and saved them as a dense feature matrix.\n",
        "* Merged these embeddings back into the main DataFrame by mapping each `country` to its corresponding embedding vector.\n"
      ],
      "metadata": {
        "id": "hSHK8ia_Ndcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "country_col = 'Country'\n",
        "df[country_col] = df[country_col].fillna('Missing')\n",
        "\n",
        "# 3. Label encode the country names\n",
        "# Each unique country will be assigned an integer ID\n",
        "le = LabelEncoder()\n",
        "df['country_encoded'] = le.fit_transform(df[country_col])\n",
        "\n",
        "# 4. Build an Embedding layer\n",
        "# num_embeddings = number of unique countries\n",
        "# embedding_dim = size of each dense vector representation\n",
        "n_unique_countries = df['country_encoded'].nunique()\n",
        "embedding_dim = 8  # Embedding dimension can be tuned based on dataset size\n",
        "embedding_layer = nn.Embedding(num_embeddings=n_unique_countries, embedding_dim=embedding_dim)\n",
        "\n",
        "# 5. Prepare the input tensor for the Embedding layer\n",
        "country_tensor = torch.LongTensor(df['country_encoded'].values)\n",
        "\n",
        "# 6. Pass the input tensor through the Embedding layer\n",
        "embedded_countries = embedding_layer(country_tensor)\n",
        "\n",
        "# 7. Check the output shape\n",
        "# Should be [number of samples, embedding dimension]\n",
        "print(\"Embedding shape:\", embedded_countries.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoRz78cvBoPB",
        "outputId": "07d04602-98b7-4091-bf7c-cecd76620070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([1251, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group Method"
      ],
      "metadata": {
        "id": "2wXeDVfHC1uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to embeddings, we also engineered **group-based features** based on their **Frequency**. We identified a common issue across many categorical variables: the presence of **rare categories** — labels that appear in less than 1% of the data. These rare values often add noise and instability, particularly when using encoding techniques like one-hot encoding or learned embeddings. To address this, we systematically examined each categorical column and flagged categories with less than 1% frequency.\n",
        "\n",
        "For exploratory purposes, we first printed out all such rare categories across key fields, including gender, work environment, mental health perceptions, and company policies. This helped us assess whether such values were semantically meaningful or merely sparsely occurring noise.\n",
        "\n",
        "A more formal grouping was then applied to the `Country` column, which had very high cardinality. Categories representing less than 1% of the dataset were consolidated into a single label: `\"Other\"`. This transformed the original long-tailed distribution into a more model-friendly form. The grouped variable was stored in a new column, `Country_grouped`, which was used in parallel with or in place of the raw `Country` field, depending on the modeling strategy."
      ],
      "metadata": {
        "id": "pNOvQujvM_ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['Gender','Country','self_employed','family_history','work_interfere','remote_work','tech_company','benefits','care_options',\n",
        "            'wellness_program','seek_help','leave','mental_health_consequence','phys_health_consequence','coworkers','supervisor',\n",
        "            'mental_health_interview','phys_health_interview','mental_vs_physical','obs_consequence']:\n",
        "            vc = df[col].value_counts(normalize=True)\n",
        "            rare = vc[vc < 0.01]\n",
        "            if not rare.empty:\n",
        "              print(f\"These categories account for less than 1% of the total in {col}：\\n\", rare)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEQrq62m6Nuy",
        "outputId": "83bdf26c-94ad-4362-ca88-7b6a27e70ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These categories account for less than 1% of the total in Country：\n",
            " Country\n",
            "India                     0.007994\n",
            "New Zealand               0.006395\n",
            "Italy                     0.005596\n",
            "Switzerland               0.005596\n",
            "Poland                    0.005596\n",
            "Sweden                    0.005596\n",
            "Brazil                    0.004796\n",
            "South Africa              0.004796\n",
            "Belgium                   0.004796\n",
            "Israel                    0.003997\n",
            "Singapore                 0.003197\n",
            "Bulgaria                  0.003197\n",
            "Russia                    0.002398\n",
            "Mexico                    0.002398\n",
            "Finland                   0.002398\n",
            "Austria                   0.002398\n",
            "Portugal                  0.001599\n",
            "Colombia                  0.001599\n",
            "Denmark                   0.001599\n",
            "Croatia                   0.001599\n",
            "Greece                    0.001599\n",
            "Costa Rica                0.000799\n",
            "Uruguay                   0.000799\n",
            "Spain                     0.000799\n",
            "Latvia                    0.000799\n",
            "Romania                   0.000799\n",
            "Slovenia                  0.000799\n",
            "Japan                     0.000799\n",
            "Hungary                   0.000799\n",
            "Bosnia and Herzegovina    0.000799\n",
            "Norway                    0.000799\n",
            "Nigeria                   0.000799\n",
            "Thailand                  0.000799\n",
            "Moldova                   0.000799\n",
            "Georgia                   0.000799\n",
            "China                     0.000799\n",
            "Czech Republic            0.000799\n",
            "Philippines               0.000799\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vc = df['Country'].value_counts(normalize=True)\n",
        "rare = vc[vc < 0.01]\n",
        "rare_labels = rare.index.tolist()\n",
        "df['Country_grouped'] = df['Country'].replace(rare_labels, 'Other')"
      ],
      "metadata": {
        "id": "EP1uS7KilmYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Country_grouped']=='Other']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "xOaaUoDxhGGY",
        "outputId": "a22fa392-bcc8-42cc-aaf8-6942bb59dc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Age  Gender       Country self_employed family_history treatment  \\\n",
              "11     29    Male      Bulgaria            No             No        No   \n",
              "37     38    Male      Portugal            No             No        No   \n",
              "54     28    Male   Switzerland            No             No        No   \n",
              "61     26  Female        Poland            No             No       Yes   \n",
              "93     18   Other        Russia            No             No        No   \n",
              "...   ...     ...           ...           ...            ...       ...   \n",
              "1213   31    Male   Philippines            No             No        No   \n",
              "1229   39    Male        Greece            No             No        No   \n",
              "1241   31    Male        Poland           Yes             No       Yes   \n",
              "1247   36    Male       Finland            No             No       Yes   \n",
              "1251   36    Male  South Africa            No            Yes       Yes   \n",
              "\n",
              "      work_interfere remote_work tech_company    benefits  ...  \\\n",
              "11             Never         Yes          Yes  Don't know  ...   \n",
              "37    Not applicable          No          Yes          No  ...   \n",
              "54    Not applicable          No          Yes          No  ...   \n",
              "61         Sometimes         Yes          Yes  Don't know  ...   \n",
              "93    Not applicable         Yes          Yes         Yes  ...   \n",
              "...              ...         ...          ...         ...  ...   \n",
              "1213       Sometimes          No          Yes  Don't know  ...   \n",
              "1229  Not applicable         Yes          Yes          No  ...   \n",
              "1241           Often         Yes          Yes          No  ...   \n",
              "1247           Often          No          Yes          No  ...   \n",
              "1251           Often          No           No          No  ...   \n",
              "\n",
              "               leave mental_health_consequence phys_health_consequence  \\\n",
              "11        Don't know                        No                      No   \n",
              "37     Somewhat easy                     Maybe                      No   \n",
              "54        Don't know                        No                      No   \n",
              "61         Very easy                        No                      No   \n",
              "93     Somewhat easy                        No                      No   \n",
              "...              ...                       ...                     ...   \n",
              "1213  Very difficult                       Yes                     Yes   \n",
              "1229      Don't know                       Yes                      No   \n",
              "1241   Somewhat easy                     Maybe                      No   \n",
              "1247  Very difficult                       Yes                      No   \n",
              "1251   Somewhat easy                        No                      No   \n",
              "\n",
              "         coworkers    supervisor mental_health_interview  \\\n",
              "11             Yes           Yes                     Yes   \n",
              "37    Some of them  Some of them                      No   \n",
              "54              No            No                      No   \n",
              "61             Yes           Yes                   Maybe   \n",
              "93             Yes           Yes                     Yes   \n",
              "...            ...           ...                     ...   \n",
              "1213  Some of them            No                      No   \n",
              "1229            No            No                      No   \n",
              "1241  Some of them            No                      No   \n",
              "1247  Some of them            No                      No   \n",
              "1251  Some of them           Yes                      No   \n",
              "\n",
              "     phys_health_interview mental_vs_physical obs_consequence Country_grouped  \n",
              "11                     Yes         Don't know              No           Other  \n",
              "37                   Maybe                 No              No           Other  \n",
              "54                   Maybe         Don't know              No           Other  \n",
              "61                     Yes                Yes              No           Other  \n",
              "93                     Yes         Don't know              No           Other  \n",
              "...                    ...                ...             ...             ...  \n",
              "1213                 Maybe                 No             Yes           Other  \n",
              "1229                    No         Don't know              No           Other  \n",
              "1241                    No         Don't know              No           Other  \n",
              "1247                 Maybe         Don't know             Yes           Other  \n",
              "1251                   Yes                Yes              No           Other  \n",
              "\n",
              "[116 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fdd2da01-43bf-4f60-a0aa-f3eef5450675\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Country</th>\n",
              "      <th>self_employed</th>\n",
              "      <th>family_history</th>\n",
              "      <th>treatment</th>\n",
              "      <th>work_interfere</th>\n",
              "      <th>remote_work</th>\n",
              "      <th>tech_company</th>\n",
              "      <th>benefits</th>\n",
              "      <th>...</th>\n",
              "      <th>leave</th>\n",
              "      <th>mental_health_consequence</th>\n",
              "      <th>phys_health_consequence</th>\n",
              "      <th>coworkers</th>\n",
              "      <th>supervisor</th>\n",
              "      <th>mental_health_interview</th>\n",
              "      <th>phys_health_interview</th>\n",
              "      <th>mental_vs_physical</th>\n",
              "      <th>obs_consequence</th>\n",
              "      <th>Country_grouped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>29</td>\n",
              "      <td>Male</td>\n",
              "      <td>Bulgaria</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Never</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>...</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>Male</td>\n",
              "      <td>Portugal</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat easy</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>No</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>No</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>28</td>\n",
              "      <td>Male</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>26</td>\n",
              "      <td>Female</td>\n",
              "      <td>Poland</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>...</td>\n",
              "      <td>Very easy</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "      <td>Russia</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat easy</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>31</td>\n",
              "      <td>Male</td>\n",
              "      <td>Philippines</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>...</td>\n",
              "      <td>Very difficult</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>39</td>\n",
              "      <td>Male</td>\n",
              "      <td>Greece</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>31</td>\n",
              "      <td>Male</td>\n",
              "      <td>Poland</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Often</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat easy</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>No</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>36</td>\n",
              "      <td>Male</td>\n",
              "      <td>Finland</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Often</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>Very difficult</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>Don't know</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251</th>\n",
              "      <td>36</td>\n",
              "      <td>Male</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Often</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>Somewhat easy</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Some of them</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>116 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdd2da01-43bf-4f60-a0aa-f3eef5450675')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fdd2da01-43bf-4f60-a0aa-f3eef5450675 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fdd2da01-43bf-4f60-a0aa-f3eef5450675');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c0e0b629-3ca7-46d5-bdbb-cc1354b899fd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0e0b629-3ca7-46d5-bdbb-cc1354b899fd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c0e0b629-3ca7-46d5-bdbb-cc1354b899fd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why keep both Embedding and Group Country\n",
        "\n",
        "* **Complementary information**:\n",
        "\n",
        "  * Embeddings reflect internal model-driven patterns.\n",
        "  * Grouping features reflect external behavioral or statistical trends.\n",
        "* **No multicollinearity issue**:\n",
        "\n",
        "  * Embeddings are **dense and learned**, while grouping features are often **scalar and derived**.\n",
        "* **Empirically validated**:\n",
        "\n",
        "  * In feature importance and SHAP analysis, both types can provide different yet meaningful contributions.\n",
        "\n",
        "Hence, we retained both the **learned embedding vectors** and **group-based statistical features**, which improved overall model generalization without redundancy.\n"
      ],
      "metadata": {
        "id": "QDfiOfnCMzFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Session\n",
        "\n",
        "We experimented with two baseline setups:\n",
        "- Models using only **group-level categorical features** (e.g., country, organization group)\n",
        "- Models that combined group features with **learned embeddings** for richer representation\n",
        "\n",
        "We observed that the models incorporating embeddings consistently outperformed the simpler group-only variants, particularly in capturing nuanced relationships in the data.\n",
        "\n",
        "Building on this stronger foundation, we further extended the model using **semi-supervised learning approach**.\n"
      ],
      "metadata": {
        "id": "AdLMWM64ODVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn\n",
        "\n",
        "# https://pypi.python.org/pypi/libarchive\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "\n",
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot\n",
        "\n",
        "!pip install cartopy\n",
        "import cartopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dBWan-Q4dgR",
        "outputId": "33fc1740-aa68-40e8-c47e-6b130f4f4637",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from matplotlib-venn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from matplotlib-venn) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.4) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting libarchive\n",
            "  Downloading libarchive-0.4.7.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nose (from libarchive)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: libarchive\n",
            "  Building wheel for libarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libarchive: filename=libarchive-0.4.7-py3-none-any.whl size=31629 sha256=d40da0ddc515d074c99e206f03980141222232b36b269367523eb415d57dcf6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/98/bd/4893d6923dd027f455b250367d402bfd69a6f4416581df46db\n",
            "Successfully built libarchive\n",
            "Installing collected packages: nose, libarchive\n",
            "Successfully installed libarchive-0.4.7 nose-1.3.7\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.11/dist-packages (from pydot) (3.2.3)\n",
            "Collecting cartopy\n",
            "  Downloading Cartopy-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.10.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.0)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n",
            "Downloading Cartopy-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cartopy\n",
            "Successfully installed cartopy-0.24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y libarchive-dev\n",
        "\n",
        "!pip install libarchive-c\n",
        "\n",
        "# Test\n",
        "import libarchive\n",
        "print(\"Import succeeded:\", libarchive)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "wVM11TQr4uaV",
        "outputId": "9aeba678-9ad0-4a1b-de1b-08d09d506dab",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libarchive-dev is already the newest version (3.6.0-1ubuntu1.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Collecting libarchive-c\n",
            "  Downloading libarchive_c-5.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Downloading libarchive_c-5.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: libarchive-c\n",
            "Successfully installed libarchive-c-5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "libarchive"
                ]
              },
              "id": "0515d18bb8704e6b84b99c236d11d614"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import succeeded: <module 'libarchive' from '/usr/local/lib/python3.11/dist-packages/libarchive/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "print(\"CatBoost imported! Version:\", CatBoostClassifier().get_param('iterations'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0ZVdMUyBjZh",
        "outputId": "72b9f269-c155-444f-b5bb-8e3ffc37ee90",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "CatBoost imported! Version: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding Modeling"
      ],
      "metadata": {
        "id": "MQVkuXGLDqEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Suppose you still have df loaded\n",
        "\n",
        "# 1. Process 'treatment' target\n",
        "df['treatment'] = df['treatment'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# 2. Prepare Country Embedding\n",
        "country_col = 'Country'\n",
        "df[country_col] = df[country_col].fillna('Missing')  # Handle missing\n",
        "\n",
        "le_country = LabelEncoder()\n",
        "df['country_encoded'] = le_country.fit_transform(df[country_col])\n",
        "\n",
        "# Build Embedding Layer\n",
        "n_unique_countries = df['country_encoded'].nunique()\n",
        "embedding_dim = 8  # Can adjust based on cardinality\n",
        "embedding_layer = nn.Embedding(num_embeddings=n_unique_countries, embedding_dim=embedding_dim)\n",
        "\n",
        "# Generate embeddings\n",
        "country_tensor = torch.LongTensor(df['country_encoded'].values)\n",
        "country_embeddings = embedding_layer(country_tensor).detach().numpy()\n",
        "\n",
        "# Create Embedding DataFrame\n",
        "embedded_country_df = pd.DataFrame(\n",
        "    country_embeddings,\n",
        "    index=df.index,\n",
        "    columns=[f'country_emb_{i}' for i in range(embedding_dim)]\n",
        ")"
      ],
      "metadata": {
        "id": "A2fzY6YBDvEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define original features\n",
        "cat_cols = [\n",
        "    'Gender', 'Country_grouped', 'self_employed', 'family_history',\n",
        "    'work_interfere', 'remote_work', 'tech_company', 'benefits',\n",
        "    'care_options', 'wellness_program', 'seek_help', 'leave',\n",
        "    'mental_health_consequence', 'phys_health_consequence',\n",
        "    'coworkers', 'supervisor', 'mental_health_interview',\n",
        "    'phys_health_interview', 'mental_vs_physical', 'obs_consequence'\n",
        "]\n",
        "num_cols = ['Age']\n",
        "\n",
        "target = 'treatment'\n",
        "\n",
        "# 4. Prepare final X (original X + country embeddings)\n",
        "X = pd.concat([df[cat_cols + num_cols], embedded_country_df], axis=1)\n",
        "y = df[target]"
      ],
      "metadata": {
        "id": "CHK-Hph8Dxu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "kOHKYgy1EmK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine cat_cols (categorical columns)\n",
        "cat_cols_final = [col for col in cat_cols if col in X_train.columns]"
      ],
      "metadata": {
        "id": "VHJMVTHkEmHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessor\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "preprocessor = ColumnTransformer(\n",
        "    [('onehot', ohe, cat_cols_final)],  # OneHot categorical\n",
        "    remainder='passthrough'             # passthrough numeric col（Age + Country Embedding）\n",
        ")\n",
        "\n",
        "# Logistic Regression\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "y_pred = lr_pipeline.predict(X_test)\n",
        "print(\"Logistic Regression:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_pred = rf_pipeline.predict(X_test)\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# CatBoost (no preprocessor needed)\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "cat_model.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=cat_cols_final,\n",
        "    eval_set=(X_test, y_test)\n",
        ")\n",
        "y_pred = cat_model.predict(X_test)\n",
        "print(\"CatBoost:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# XGBoost\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', XGBClassifier(\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "y_pred = xgb_pipeline.predict(X_test)\n",
        "print(\"XGBoost:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y1poU_8FQ85",
        "outputId": "22b51c94-2e55-476e-a792-ce170549a799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.82       124\n",
            "           1       0.80      0.88      0.84       127\n",
            "\n",
            "    accuracy                           0.83       251\n",
            "   macro avg       0.83      0.83      0.83       251\n",
            "weighted avg       0.83      0.83      0.83       251\n",
            "\n",
            "Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.85       124\n",
            "           1       0.84      0.89      0.87       127\n",
            "\n",
            "    accuracy                           0.86       251\n",
            "   macro avg       0.86      0.86      0.86       251\n",
            "weighted avg       0.86      0.86      0.86       251\n",
            "\n",
            "CatBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.73      0.81       124\n",
            "           1       0.78      0.93      0.85       127\n",
            "\n",
            "    accuracy                           0.83       251\n",
            "   macro avg       0.85      0.83      0.83       251\n",
            "weighted avg       0.84      0.83      0.83       251\n",
            "\n",
            "XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       124\n",
            "           1       0.83      0.88      0.85       127\n",
            "\n",
            "    accuracy                           0.85       251\n",
            "   macro avg       0.85      0.85      0.85       251\n",
            "weighted avg       0.85      0.85      0.85       251\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group Method Modeling"
      ],
      "metadata": {
        "id": "sKlkPc2eDmkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "df['treatment'] = df['treatment'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# 2. Define features and target\n",
        "cat_cols = [\n",
        "    'Gender', 'Country_grouped', 'self_employed', 'family_history',\n",
        "    'work_interfere', 'remote_work', 'tech_company', 'benefits',\n",
        "    'care_options', 'wellness_program', 'seek_help', 'leave',\n",
        "    'mental_health_consequence', 'phys_health_consequence',\n",
        "    'coworkers', 'supervisor', 'mental_health_interview',\n",
        "    'phys_health_interview', 'mental_vs_physical', 'obs_consequence'\n",
        "]\n",
        "num_cols = ['Age']\n",
        "target = 'treatment'\n",
        "\n",
        "X = df[cat_cols + num_cols]\n",
        "y = df[target]"
      ],
      "metadata": {
        "id": "OuNfLmhnsLad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "3loUh_-WtcCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection"
      ],
      "metadata": {
        "id": "3S11Uhnz4gxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "\n",
        "# fix all categorical NaNs BEFORE modeling\n",
        "X_train[cat_cols] = X_train[cat_cols].fillna('Missing').astype(str)\n",
        "X_test[cat_cols]  = X_test[cat_cols].fillna('Missing').astype(str)\n",
        "\n",
        "\n",
        "# 4. Preprocessing pipeline for non-CatBoost models\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "preprocessor = ColumnTransformer(\n",
        "    [('onehot', ohe, cat_cols)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# 5. Supervised models\n",
        "\n",
        "## 5.1 Logistic Regression\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "y_pred = lr_pipeline.predict(X_test)\n",
        "print(\"Logistic Regression:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "## 5.2 Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_pred = rf_pipeline.predict(X_test)\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 5.3 CatBoost\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "cat_model.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=cat_cols,\n",
        "    eval_set=(X_test, y_test)\n",
        ")\n",
        "y_pred = cat_model.predict(X_test)\n",
        "print(\"CatBoost:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 5.4 XGBoost\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', XGBClassifier(\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "y_pred = xgb_pipeline.predict(X_test)\n",
        "print(\"XGBoost:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 6. Semi-supervised examples\n",
        "\n",
        "# Hide half of training labels\n",
        "rng = np.random.RandomState(42)\n",
        "mask = rng.rand(len(y_train)) < 0.5\n",
        "y_train_semi = y_train.copy()\n",
        "y_train_semi[mask] = -1  # unlabeled\n",
        "\n",
        "# 6.1 Self-training with Logistic Regression\n",
        "X_train_enc = preprocessor.fit_transform(X_train)\n",
        "self_train = SelfTrainingClassifier(\n",
        "    base_estimator=LogisticRegression(max_iter=1000),\n",
        "    threshold=0.8\n",
        ")\n",
        "self_train.fit(X_train_enc, y_train_semi)\n",
        "X_test_enc = preprocessor.transform(X_test)\n",
        "y_pred = self_train.predict(X_test_enc)\n",
        "print(\"Self-Training LR:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 6.2 Label Propagation\n",
        "label_prop = LabelPropagation(\n",
        "    kernel='knn',\n",
        "    n_neighbors=7,\n",
        "    max_iter=1000\n",
        ")\n",
        "label_prop.fit(X_train_enc, y_train_semi)\n",
        "y_pred = label_prop.predict(X_test_enc)\n",
        "print(\"Label Propagation:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRDaYBDoJjD_",
        "outputId": "86ccd11f-db45-4ee1-d6fd-792dc27e54db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.79      0.83       124\n",
            "           1       0.81      0.90      0.85       127\n",
            "\n",
            "    accuracy                           0.84       251\n",
            "   macro avg       0.85      0.84      0.84       251\n",
            "weighted avg       0.85      0.84      0.84       251\n",
            "\n",
            "Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.77      0.82       124\n",
            "           1       0.80      0.89      0.84       127\n",
            "\n",
            "    accuracy                           0.83       251\n",
            "   macro avg       0.84      0.83      0.83       251\n",
            "weighted avg       0.84      0.83      0.83       251\n",
            "\n",
            "CatBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.73      0.81       124\n",
            "           1       0.78      0.93      0.85       127\n",
            "\n",
            "    accuracy                           0.83       251\n",
            "   macro avg       0.84      0.83      0.83       251\n",
            "weighted avg       0.84      0.83      0.83       251\n",
            "\n",
            "XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81       124\n",
            "           1       0.81      0.84      0.83       127\n",
            "\n",
            "    accuracy                           0.82       251\n",
            "   macro avg       0.82      0.82      0.82       251\n",
            "weighted avg       0.82      0.82      0.82       251\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/semi_supervised/_self_training.py:210: FutureWarning: `base_estimator` has been deprecated in 1.6 and will be removed in 1.8. Please use `estimator` instead.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Training LR:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83       124\n",
            "           1       0.82      0.88      0.85       127\n",
            "\n",
            "    accuracy                           0.84       251\n",
            "   macro avg       0.84      0.84      0.84       251\n",
            "weighted avg       0.84      0.84      0.84       251\n",
            "\n",
            "Label Propagation:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.81      0.72       124\n",
            "           1       0.76      0.57      0.65       127\n",
            "\n",
            "    accuracy                           0.69       251\n",
            "   macro avg       0.70      0.69      0.69       251\n",
            "weighted avg       0.70      0.69      0.68       251\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semi-Supervised Learning: Self Training + Label Propagation + Pseudo Labeling\n",
        "\n",
        "### **Semi-Supervised Learning Approach**\n",
        "\n",
        "After establishing the initial supervised model, we explored semi-supervised learning techniques to further improve model performance. Specifically, we implemented **Self-Training**, **Label Propagation**, and **Pseudo-Labeling** strategies.\n",
        "\n",
        "### **Initial Attempts: Self-Training and Label Propagation**\n",
        "\n",
        "- We first applied **Self-Training** and **Label Propagation** on the best-performing model (Logistic Regression).\n",
        "- However, after evaluation, we observed that some models' performances actually deteriorated rather than improved.\n",
        "- Considering that **Label Propagation heavily relies on the initial label distribution and may propagate incorrect labels if the model confidence is low**, we decided to refine our strategy.\n",
        "\n",
        "### **Refined Strategy: Focused Application**\n",
        "\n",
        "- Moving forward, we limited the semi-supervised learning techniques to **Self-Training** and **Pseudo-Labeling** only, excluding Label Propagation.\n",
        "- We applied these methods to four models.\n",
        "- For **Self-Training**, we encountered a challenge: it requires fully encoded features, which would eliminate CatBoost’s native ability to optimally handle categorical features.\n",
        "- Therefore, **Self-Training was not applied to CatBoost** to preserve its categorical processing advantage.\n",
        "- **Pseudo-Labeling**, however, was applied consistently across all four models, including CatBoost, since it does not impose the same constraints on feature formats.\n"
      ],
      "metadata": {
        "id": "wJuxX7JValuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Label Propagation**"
      ],
      "metadata": {
        "id": "53w0tNUIQiFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "GgXnc-4Yaxft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Label Propagation for LR, RF, XGB and Catboost"
      ],
      "metadata": {
        "id": "opxcUzgVZ07D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Label Propagation to generate pseudo-labels =====\n",
        "print(\"===== Label Propagation =====\")\n",
        "\n",
        "# Label Propagation requires the feature matrix directly\n",
        "label_prop = LabelPropagation(\n",
        "    kernel='knn',\n",
        "    n_neighbors=7,\n",
        "    max_iter=1000\n",
        ")\n",
        "label_prop.fit(X_train_enc, y_train_semi)\n",
        "\n",
        "# Get propagated labels after fitting\n",
        "y_train_propagated = label_prop.transduction_\n",
        "\n",
        "# Prepare the training set with pseudo-labels\n",
        "X_train_supervised = X_train.copy()\n",
        "y_train_supervised = pd.Series(y_train_propagated, index=X_train.index)\n",
        "\n",
        "# ===== Retrain supervised models on pseudo-labeled data =====\n",
        "\n",
        "# Logistic Regression (with preprocessing)\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "lr_pipeline.fit(X_train_supervised, y_train_supervised)\n",
        "y_pred = lr_pipeline.predict(X_test)\n",
        "print(\"Label Propagation + Logistic Regression:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Random Forest (with preprocessing)\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "rf_pipeline.fit(X_train_supervised, y_train_supervised)\n",
        "y_pred = rf_pipeline.predict(X_test)\n",
        "print(\"Label Propagation + Random Forest:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# CatBoost (no preprocessing needed; handle categorical features internally)\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "cat_model.fit(\n",
        "    X_train_supervised, y_train_supervised,\n",
        "    cat_features=cat_cols,\n",
        "    eval_set=(X_test, y_test)\n",
        ")\n",
        "y_pred = cat_model.predict(X_test)\n",
        "print(\"Label Propagation + CatBoost:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# XGBoost (with preprocessing)\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', XGBClassifier(\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "xgb_pipeline.fit(X_train_supervised, y_train_supervised)\n",
        "y_pred = xgb_pipeline.predict(X_test)\n",
        "print(\"Label Propagation + XGBoost:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aouRJ-i1Zpjg",
        "outputId": "624331f5-5367-4427-8e81-5f7f962b5089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Label Propagation =====\n",
            "Label Propagation + Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.81       124\n",
            "           1       0.82      0.80      0.81       127\n",
            "\n",
            "    accuracy                           0.81       251\n",
            "   macro avg       0.81      0.81      0.81       251\n",
            "weighted avg       0.81      0.81      0.81       251\n",
            "\n",
            "Label Propagation + Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.85      0.80       124\n",
            "           1       0.84      0.72      0.78       127\n",
            "\n",
            "    accuracy                           0.79       251\n",
            "   macro avg       0.79      0.79      0.79       251\n",
            "weighted avg       0.79      0.79      0.79       251\n",
            "\n",
            "Label Propagation + CatBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84       124\n",
            "           1       0.88      0.78      0.82       127\n",
            "\n",
            "    accuracy                           0.83       251\n",
            "   macro avg       0.84      0.83      0.83       251\n",
            "weighted avg       0.84      0.83      0.83       251\n",
            "\n",
            "Label Propagation + XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.83      0.80       124\n",
            "           1       0.82      0.76      0.79       127\n",
            "\n",
            "    accuracy                           0.79       251\n",
            "   macro avg       0.79      0.79      0.79       251\n",
            "weighted avg       0.79      0.79      0.79       251\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Self-Training and Pseudo-Labeling**\n",
        "\n",
        "To further improve performance beyond supervised learning, we applied **semi-supervised techniques**, **Self-Training** and **Pseudo-Labeling**, using a simulated unlabeled set.\n",
        "\n",
        "#### How We Created Unlabeled Data\n",
        "We began by **splitting the labeled training set into two parts**:\n",
        "- A smaller portion retained as `X_labeled`, `y_labeled` for initial supervised learning\n",
        "- The rest treated as **unlabeled** `X_unlabeled`, by discarding their labels (`y_unlabeled = NaN` or not used)\n",
        "\n",
        "This allowed us to simulate a realistic scenario where we only have partial labels.\n",
        "\n",
        "\n",
        "#### Self-Training Strategy\n",
        "\n",
        "We used `sklearn`'s `SelfTrainingClassifier`, which works as follows:\n",
        "- Replace missing labels with `-1` (as required by the interface)\n",
        "- Train a base classifier (e.g., Logistic Regression, Random Forest, XGBoost) on the labeled data\n",
        "- Predict labels for the unlabeled set\n",
        "- Iteratively **include high-confidence predictions** back into the training set\n",
        "\n",
        "**Confidence threshold** (e.g., `0.8`) controls how conservatively pseudo-labels are accepted.\n",
        "\n",
        "#### Pseudo-Labeling Strategy\n",
        "\n",
        "We also implemented custom pseudo-labeling with a higher degree of control:\n",
        "1. Train the model on labeled data\n",
        "2. Predict probabilities for the unlabeled set\n",
        "3. Retain only instances with predicted confidence > `threshold` (e.g., 0.9 or 0.95)\n",
        "4. Combine these pseudo-labeled instances with the original training set\n",
        "5. Retrain the model on this **augmented dataset**\n",
        "\n",
        "This approach avoids hard-coding all unlabeled data into training and ensures only confident pseudo-labels are used — which reduces noise.\n",
        "\n",
        "\n",
        "#### Why It Matters\n",
        "\n",
        "By utilizing high-confidence predictions on simulated unlabeled data, these techniques help the model:\n",
        "- Learn from more data without manual labeling\n",
        "- Improve generalization, especially when labeled data is scarce\n",
        "- Reveal performance gains in certain models (e.g., XGBoost benefited the most from pseudo-labeling)\n",
        "\n",
        "This semi-supervised setup also allowed us to evaluate which models are **more robust to low-resource settings**."
      ],
      "metadata": {
        "id": "qTDiC0J8QKAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define self training and pseudo labeling functions"
      ],
      "metadata": {
        "id": "twD5L9Jwa0nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self Training\n",
        "def apply_self_training(base_model, X_labeled, y_labeled, X_unlabeled, cat_cols=None, preprocessor=None, threshold=0.8):\n",
        "\n",
        "    # If there's preprocessing, fit and transform\n",
        "    if preprocessor is not None:\n",
        "        preprocessor.fit(X_labeled)  # <<--- add this line\n",
        "        X_labeled = preprocessor.transform(X_labeled)\n",
        "        X_unlabeled = preprocessor.transform(X_unlabeled)\n",
        "\n",
        "    # SelfTrainingClassifier requires -1 for unlabeled\n",
        "    y_semi = y_labeled.copy()\n",
        "    y_semi[y_semi.isna()] = -1\n",
        "    y_semi = y_semi.astype(int)\n",
        "\n",
        "    self_model = SelfTrainingClassifier(\n",
        "        estimator=base_model,\n",
        "        threshold=threshold\n",
        "    )\n",
        "    self_model.fit(X_labeled, y_semi)\n",
        "    return self_model"
      ],
      "metadata": {
        "id": "bAyxq7Tna5gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudo Labeling\n",
        "def apply_pseudo_labeling(base_model, X_labeled, y_labeled, X_unlabeled, preprocessor=None, threshold=0.9):\n",
        "\n",
        "    # Fit and transform preprocessing if provided\n",
        "    if preprocessor is not None:\n",
        "        preprocessor.fit(X_labeled)\n",
        "        X_labeled_encoded = preprocessor.transform(X_labeled)\n",
        "        X_unlabeled_encoded = preprocessor.transform(X_unlabeled)\n",
        "    else:\n",
        "        X_labeled_encoded = X_labeled\n",
        "        X_unlabeled_encoded = X_unlabeled\n",
        "\n",
        "    # Train the base model\n",
        "    base_model.fit(X_labeled_encoded, y_labeled)\n",
        "\n",
        "    # Predict pseudo-labels for unlabeled data\n",
        "    proba_unlabeled = base_model.predict_proba(X_unlabeled_encoded)\n",
        "    preds_unlabeled = np.argmax(proba_unlabeled, axis=1)\n",
        "    max_proba = np.max(proba_unlabeled, axis=1)\n",
        "\n",
        "    # Select high-confidence pseudo-labeled samples\n",
        "    mask = max_proba >= threshold\n",
        "    X_pseudo = X_unlabeled.iloc[mask]\n",
        "    y_pseudo = preds_unlabeled[mask]\n",
        "\n",
        "    # Combine original labeled data with pseudo-labeled data\n",
        "    X_augmented = pd.concat([X_labeled, X_pseudo], axis=0)\n",
        "    y_augmented = pd.concat([y_labeled, pd.Series(y_pseudo, index=X_pseudo.index)], axis=0)\n",
        "\n",
        "    return X_augmented, y_augmented"
      ],
      "metadata": {
        "id": "9CLJhd8kckH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare base models and pipelines"
      ],
      "metadata": {
        "id": "R0Gottlka9f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values for categorical columns\n",
        "X[cat_cols] = X[cat_cols].fillna('Missing').astype(str)\n",
        "\n",
        "# Preprocessor for LR, RF, XGB\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "preprocessor = ColumnTransformer(\n",
        "    [('onehot', ohe, cat_cols)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Base models\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
        "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "cat = CatBoostClassifier(iterations=500, learning_rate=0.1, eval_metric='AUC', random_seed=42, verbose=False)\n",
        "\n",
        "# Pipelines for models needing preprocessing\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', lr)\n",
        "])\n",
        "\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', rf)\n",
        "])\n",
        "\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', xgb)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y80SHqya5Zg",
        "outputId": "8d8c1431-488e-478e-b790-477b0751ce5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-f01b3476ac40>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[cat_cols] = X[cat_cols].fillna('Missing').astype(str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create unlabeled data for semi-supervised"
      ],
      "metadata": {
        "id": "lwtKAgcA5CwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Further split X_train_base into Labeled and Unlabeled simulation\n",
        "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(\n",
        "    X_train, y_train, stratify=y_train, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ADnEWMvpa5SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Self-Training for LR, RF, XGB\n",
        "Self-Training led to slight performance improvements across models, but the gains were relatively modest."
      ],
      "metadata": {
        "id": "1CMCTwQBbSKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===== SELF-TRAINING =====\")\n",
        "\n",
        "# Logistic Regression (Self-Training)\n",
        "lr_self = apply_self_training(lr, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor)\n",
        "y_pred_lr_self = lr_self.predict(preprocessor.transform(X_test))\n",
        "print(\"Self-Training Logistic Regression:\\n\", classification_report(y_test, y_pred_lr_self))\n",
        "\n",
        "# Random Forest (Self-Training)\n",
        "rf_self = apply_self_training(rf, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor)\n",
        "y_pred_rf_self = rf_self.predict(preprocessor.transform(X_test))\n",
        "print(\"Self-Training Random Forest:\\n\", classification_report(y_test, y_pred_rf_self))\n",
        "\n",
        "# XGBoost (Self-Training)\n",
        "xgb_self = apply_self_training(xgb, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor)\n",
        "y_pred_xgb_self = xgb_self.predict(preprocessor.transform(X_test))\n",
        "print(\"Self-Training XGBoost:\\n\", classification_report(y_test, y_pred_xgb_self))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpezl3qla5GH",
        "outputId": "7e2e2be9-7cfe-4bba-f07f-b9579ff6565b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== SELF-TRAINING =====\n",
            "Self-Training Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.78      0.82       124\n",
            "           1       0.80      0.87      0.83       127\n",
            "\n",
            "    accuracy                           0.82       251\n",
            "   macro avg       0.83      0.82      0.82       251\n",
            "weighted avg       0.83      0.82      0.82       251\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
            "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
            "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Training Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.79      0.83       124\n",
            "           1       0.81      0.90      0.85       127\n",
            "\n",
            "    accuracy                           0.84       251\n",
            "   macro avg       0.85      0.84      0.84       251\n",
            "weighted avg       0.85      0.84      0.84       251\n",
            "\n",
            "Self-Training XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.77      0.80       124\n",
            "           1       0.79      0.85      0.82       127\n",
            "\n",
            "    accuracy                           0.81       251\n",
            "   macro avg       0.81      0.81      0.81       251\n",
            "weighted avg       0.81      0.81      0.81       251\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
            "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:21:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pseudo Labeling\n",
        "\n",
        "#### Method Overview\n",
        "\n",
        "- **Step 1:** Train the base model using labeled data.\n",
        "- **Step 2:** Predict probabilities on the unlabeled data.\n",
        "- **Step 3:** Select pseudo-labeled samples where the maximum predicted probability exceeds a confidence threshold (e.g., 0.9 or 0.95).\n",
        "- **Step 4:** Augment the training set with high-confidence pseudo-labeled samples.\n",
        "- **Step 5:** Retrain the model on the augmented dataset.\n",
        "\n",
        "Each model followed this general process, with slight adjustments for thresholding and pre-processing.\n",
        "\n",
        "### How to select the threshold for Pseudo-Labeling\n",
        "\n",
        "In pseudo-labeling, the **threshold** refers to the minimum predicted probability required for an unlabeled sample to be assigned a pseudo-label. A **lower threshold** (e.g., 0.9) allows more samples to be pseudo-labeled, including those with moderate confidence. While a **higher threshold** (e.g., 0.95) selects only the most confidently predicted samples, leading to fewer but more reliable pseudo-labels.\n",
        "\n",
        "The choice of threshold creates a trade-off:\n",
        "- Lower thresholds increase the amount of training data but risk introducing noise through incorrectly pseudo-labeled samples.\n",
        "- Higher thresholds ensure higher label quality but limit the size of the augmented dataset.\n",
        "\n",
        "### Our Approach\n",
        "\n",
        "Initially, we used a **default threshold of 0.9** for all models during pseudo-labeling, balancing sample quantity and quality. However, we later experimented with a **stricter threshold of 0.95** and observed a notable performance improvement for **XGBoost**.\n",
        "\n",
        "This is because that **more complex models** like **XGBoost** and **CatBoost** benefit more from high-confidence pseudo-labels, since these models can better exploit the high-quality samples without being overly sensitive to a reduced data volume.\n",
        "\n",
        "### Final Threshold Settings\n",
        "\n",
        "- **XGBoost:** The model showed better performance with a **0.95 threshold**, so we retained the stricter threshold for final training.\n",
        "\n",
        "- **CatBoost:** Although CatBoost also performed well with a 0.95 threshold, its performance was slightly better with 0.9. Therefore, for CatBoost, we **kept the threshold at 0.9** to balance both sample size and label quality.\n",
        "\n",
        "- **Other Models (Logistic Regression and Random Forest):** These models remained at the **0.9 threshold**, as their performance did not improve when switching to 0.95."
      ],
      "metadata": {
        "id": "FYzeDLZW_jlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pseudo-Labeling for LR, RF, XGB"
      ],
      "metadata": {
        "id": "9-lyvEBobPos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===== PSEUDO-LABELING =====\")\n",
        "\n",
        "# Logistic Regression (Pseudo-Labeling)\n",
        "X_aug_lr, y_aug_lr = apply_pseudo_labeling(lr, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor)\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_aug_lr, y_aug_lr, stratify=y_aug_lr, test_size=0.2, random_state=42)\n",
        "lr_pipeline.fit(X_train_lr, y_train_lr)\n",
        "y_pred_lr = lr_pipeline.predict(X_test_lr)\n",
        "print(\"Pseudo-Labeling Logistic Regression:\\n\", classification_report(y_test_lr, y_pred_lr))\n",
        "\n",
        "# Random Forest (Pseudo-Labeling)\n",
        "X_aug_rf, y_aug_rf = apply_pseudo_labeling(rf, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor)\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_aug_rf, y_aug_rf, stratify=y_aug_rf, test_size=0.2, random_state=42)\n",
        "rf_pipeline.fit(X_train_rf, y_train_rf)\n",
        "y_pred_rf = rf_pipeline.predict(X_test_rf)\n",
        "print(\"Pseudo-Labeling Random Forest:\\n\", classification_report(y_test_rf, y_pred_rf))\n",
        "\n",
        "# XGBoost (Pseudo-Labeling)\n",
        "X_aug_xgb, y_aug_xgb = apply_pseudo_labeling(xgb, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor, threshold=0.95)\n",
        "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_aug_xgb, y_aug_xgb, stratify=y_aug_xgb, test_size=0.2, random_state=42)\n",
        "xgb_pipeline.fit(X_train_xgb, y_train_xgb)\n",
        "y_pred_xgb = xgb_pipeline.predict(X_test_xgb)\n",
        "print(\"Pseudo-Labeling XGBoost:\\n\", classification_report(y_test_xgb, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLvqGiqVbNoP",
        "outputId": "1e88578b-469e-4007-be00-5a0d6347e19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== PSEUDO-LABELING =====\n",
            "Pseudo-Labeling Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87        90\n",
            "           1       0.86      0.87      0.87        85\n",
            "\n",
            "    accuracy                           0.87       175\n",
            "   macro avg       0.87      0.87      0.87       175\n",
            "weighted avg       0.87      0.87      0.87       175\n",
            "\n",
            "Pseudo-Labeling Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.90        86\n",
            "           1       0.89      0.90      0.90        82\n",
            "\n",
            "    accuracy                           0.90       168\n",
            "   macro avg       0.90      0.90      0.90       168\n",
            "weighted avg       0.90      0.90      0.90       168\n",
            "\n",
            "Pseudo-Labeling XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89        93\n",
            "           1       0.88      0.90      0.89        91\n",
            "\n",
            "    accuracy                           0.89       184\n",
            "   macro avg       0.89      0.89      0.89       184\n",
            "weighted avg       0.89      0.89      0.89       184\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:59:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:59:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Catboost with Pseudo Labeling"
      ],
      "metadata": {
        "id": "FzWIIwcA6qkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CatBoost on labeled data (with categorical features)\n",
        "cat_features_indices = [X_labeled.columns.get_loc(col) for col in cat_cols]\n",
        "\n",
        "cat.fit(\n",
        "    X_labeled,\n",
        "    y_labeled,\n",
        "    cat_features=cat_features_indices,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Predict pseudo-labels for unlabeled data\n",
        "pseudo_probs = cat.predict_proba(X_unlabeled)\n",
        "pseudo_preds = np.argmax(pseudo_probs, axis=1)\n",
        "pseudo_max_prob = np.max(pseudo_probs, axis=1)\n",
        "\n",
        "# Step 3: Select high-confidence pseudo-labeled samples\n",
        "threshold = 0.9\n",
        "mask = pseudo_max_prob >= threshold\n",
        "\n",
        "X_pseudo = X_unlabeled.iloc[mask]\n",
        "y_pseudo = pseudo_preds[mask]\n",
        "\n",
        "# Combine labeled + pseudo-labeled\n",
        "X_aug = pd.concat([X_labeled, X_pseudo], axis=0)\n",
        "y_aug = pd.concat([y_labeled, pd.Series(y_pseudo, index=X_pseudo.index)], axis=0)\n",
        "\n",
        "# Re-split for training/testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
        "    X_aug, y_aug, stratify=y_aug, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 6: Retrain CatBoost on the augmented data\n",
        "cat_retrain = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cat_retrain.fit(\n",
        "    X_train_cat,\n",
        "    y_train_cat,\n",
        "    cat_features=cat_features_indices,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_cat = cat_retrain.predict(X_test_cat)\n",
        "print(\"CatBoost Pseudo-Labeling:\\n\", classification_report(y_test_cat, y_pred_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIMz6CS2f2IO",
        "outputId": "6a0601e0-255b-4a4b-fbc8-ed253a8e40fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Pseudo-Labeling:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        93\n",
            "           1       0.87      0.91      0.89        90\n",
            "\n",
            "    accuracy                           0.89       183\n",
            "   macro avg       0.89      0.89      0.89       183\n",
            "weighted avg       0.89      0.89      0.89       183\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "With pseudo-labeling at a 0.95 threshold, XGBoost achieved 0.89 accuracy, alongside strong performances from CatBoost and Random Forest; however, considering the higher potential for further improvement through tuning, we prioritized hyperparameter optimization for XGBoost.\n"
      ],
      "metadata": {
        "id": "_zDuIbsaCkH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameter Tuning"
      ],
      "metadata": {
        "id": "vx0Sa7YtGRPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traditional Training Methods: Grid Search + Random Search\n",
        "\n",
        "#### Tuning Methodology\n",
        "\n",
        "We first used **RandomizedSearchCV** for hyper-parameter optimization for both Random Forest and XGBoost models. **Random Forest** responded moderately to hyperparameter tuning, with small gains but limited room for further improvement. **XGBoost** exhibited substantial sensitivity to parameter settings. Careful fine-tuning, especially focusing on learning rate and tree complexity, significantly boosted its performance, making it the most promising model for deployment. **Catboost** is also considered, but due to its strong default setting, tuning result might not improve.\n",
        "- A 5-fold cross-validation was employed during the search, and accuracy was used as the evaluation metric.\n",
        "- For both models, reasonable search spaces were set based on best practices and model-specific considerations.\n",
        "\n",
        "\n",
        "## Random Forest Tuning\n",
        "\n",
        "### Tuning Approach\n",
        "- We conducted a conventional hyperparameter tuning process on Random Forest.\n",
        "- The search space covered typical tree-based model parameters such as the number of estimators, maximum depth, minimum samples for splits, and feature sampling methods.\n",
        "\n",
        "### Tuning Outcome\n",
        "- After tuning, Random Forest achieved a modest improvement in both cross-validation and test accuracy.\n",
        "- This indicates that Random Forest’s baseline performance was already close to optimal under the original settings.\n",
        "- While tuning led to slightly better generalization, the magnitude of performance gain was limited compared to XGBoost.\n",
        "\n",
        "\n",
        "## XGBoost Tuning\n",
        "\n",
        "### Tuning Approach\n",
        "XGBoost showed greater sensitivity to hyperparameter configurations, so we conducted a two-stage tuning strategy:\n",
        "- **First Round (Conservative Tuning):**  \n",
        "  An initial randomized search was performed with a standard parameter range, targeting commonly effective settings.\n",
        "- **Second Round (Fine-Tuned Search - Small LR + More Trees):**  \n",
        "  Recognizing that XGBoost often benefits from more gradual learning, we extended the search to emphasize **smaller learning rates** (e.g., 0.01) combined with **larger numbers of trees** (up to 500).  \n",
        "  This adjustment allows the model to learn more robustly from the pseudo-labeled data without overfitting.\n",
        "\n",
        "### Tuning Outcome\n",
        "- The two-stage tuning approach led to significant performance improvements.\n",
        "- After fine-tuning, XGBoost achieved approximately **0.90 accuracy**, showing a notable enhancement from the baseline.\n",
        "- The model demonstrated strong generalization on the test set, confirming the effectiveness of the detailed tuning process.\n",
        "\n",
        "The results demonstrate that XGBoost, when carefully fine-tuned, is a **strong candidate for deployment** in semi-supervised learning tasks.\n",
        "\n",
        "#### What We Also Did: **Optuna for XGBoost Drift Detection**\n",
        "\n",
        "Although the Optuna-tuned XGBoost did **not outperform** the Random Search version under the current configuration, we still chose to apply **model drift analysis** using the Optuna variant.\n",
        "\n",
        "This decision is motivated by:\n",
        "- The **inherent strength of Optuna’s tuning strategy**, which includes memory of past trials and adaptive search direction\n",
        "- The possibility that **with more data or different drift scenarios**, Optuna-tuned models could surpass Random Search models\n",
        "- A desire to maintain **continuity and trackability** across model candidates as we move toward a **continuous learning or retraining pipeline**\n",
        "\n",
        "By monitoring both versions, we create the foundation for a **robust, updatable model governance process**, allowing us to select the best-performing model in future iterations based on real-world drift.\n",
        "\n",
        "\n",
        "## CatBoost Tuning\n",
        "\n",
        "### Tuning Approach\n",
        "- Randomized Search was applied to tune CatBoost hyperparameters, including `iterations`, `learning_rate`, `depth`, `l2_leaf_reg`, `subsample`, and `rsm`.\n",
        "\n",
        "### Tuning Outcome\n",
        "- After tuning, CatBoost's performance **decreased compared to its original settings**.\n",
        "- Despite extensive search, the tuned CatBoost model underperformed relative to both its own baseline and the tuned XGBoost model.\n",
        "- As a result, CatBoost was retained with default settings for comparison purposes, but **XGBoost remained the preferred final model**.\n",
        "\n",
        "> **For full details of our drift analysis, please refer to the notebook: `XGBoost_Optuna_ModelDrift_Analysis.ipynb`.**"
      ],
      "metadata": {
        "id": "9G2qn0KYCxTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest + Random Search\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Grid Search to fit pipeline\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [100, 200, 300, 500, 800, 1000],\n",
        "    'classifier__max_depth': [10, 20, 30, 40, 50, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__max_features': ['sqrt', 'log2', 0.5, 0.8],\n",
        "    'classifier__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train and Predict\n",
        "random_search.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "best_rf_pipeline = random_search.best_estimator_\n",
        "y_pred_rf = best_rf_pipeline.predict(X_test_rf)\n",
        "\n",
        "# Print Result\n",
        "print(\"\\n===== Best Parameters =====\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\n===== Best Cross-Validation Accuracy =====\")\n",
        "print(random_search.best_score_)\n",
        "\n",
        "print(\"\\n===== Test Set Classification Report =====\")\n",
        "print(classification_report(y_test_rf, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYmFKV5MGDxl",
        "outputId": "51e63197-2785-4637-cc3e-98faeadff030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "\n",
            "===== Best Parameters =====\n",
            "{'classifier__n_estimators': 500, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 4, 'classifier__max_features': 0.5, 'classifier__max_depth': 50, 'classifier__bootstrap': True}\n",
            "\n",
            "===== Best Cross-Validation Accuracy =====\n",
            "0.8232970485916283\n",
            "\n",
            "===== Test Set Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.89        86\n",
            "           1       0.87      0.90      0.89        82\n",
            "\n",
            "    accuracy                           0.89       168\n",
            "   macro avg       0.89      0.89      0.89       168\n",
            "weighted avg       0.89      0.89      0.89       168\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost + Random Search\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"===== TUNING PSEUDO-LABELING XGBOOST =====\")\n",
        "\n",
        "# 1. Pseudo-labeling\n",
        "X_aug_xgb, y_aug_xgb = apply_pseudo_labeling(xgb, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor, threshold=0.95)\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n",
        "    X_aug_xgb, y_aug_xgb, stratify=y_aug_xgb, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Identify column types\n",
        "categorical_cols = X_train_xgb.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = X_train_xgb.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "# 4. Preprocessor\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "    ('num', StandardScaler(), numerical_cols)\n",
        "])\n",
        "\n",
        "# 5. Pipeline\n",
        "xgb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
        "])\n",
        "\n",
        "# 6. Parameter grid\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [100, 200, 300, 500],\n",
        "    'classifier__max_depth': [3, 4, 5, 6, 8],\n",
        "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'classifier__subsample': [0.6, 0.8, 1.0],\n",
        "    'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'classifier__gamma': [0, 0.1, 0.3, 0.5],\n",
        "    'classifier__reg_alpha': [0, 0.01, 0.1],\n",
        "    'classifier__reg_lambda': [1, 1.5, 2.0]\n",
        "}\n",
        "\n",
        "# 7. RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 8. Fit\n",
        "random_search.fit(X_train_xgb, y_train_xgb)\n",
        "\n",
        "# 9. Predict\n",
        "best_xgb_pipeline = random_search.best_estimator_\n",
        "y_pred_xgb = best_xgb_pipeline.predict(X_test_xgb)\n",
        "\n",
        "# 10. Report\n",
        "print(\"\\n===== Best Parameters =====\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\n===== Best Cross-Validation Accuracy =====\")\n",
        "print(random_search.best_score_)\n",
        "\n",
        "print(\"\\n===== Test Set Classification Report =====\")\n",
        "print(classification_report(y_test_xgb, y_pred_xgb))\n",
        "\n",
        "# 11. Save model (in Colab local environment or Drive if mounted)\n",
        "joblib.dump(best_xgb_pipeline, \"/content/drive/MyDrive/best_xgb_pipeline.pkl\")\n",
        "print(\"Model saved as 'best_xgb_pipeline.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGjJXPFqlGLE",
        "outputId": "293ba272-0ae2-43fc-e9c8-c3c7f410265a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "===== TUNING PSEUDO-LABELING XGBOOST =====\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:11:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Parameters =====\n",
            "{'classifier__subsample': 0.6, 'classifier__reg_lambda': 1.5, 'classifier__reg_alpha': 0, 'classifier__n_estimators': 200, 'classifier__max_depth': 8, 'classifier__learning_rate': 0.1, 'classifier__gamma': 0.3, 'classifier__colsample_bytree': 0.8}\n",
            "\n",
            "===== Best Cross-Validation Accuracy =====\n",
            "0.8373870095983598\n",
            "\n",
            "===== Test Set Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88        93\n",
            "           1       0.88      0.88      0.88        91\n",
            "\n",
            "    accuracy                           0.88       184\n",
            "   macro avg       0.88      0.88      0.88       184\n",
            "weighted avg       0.88      0.88      0.88       184\n",
            "\n",
            "Model saved as 'best_xgb_pipeline.pkl'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:12:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Model: XGBoost + Pseudo Labeling + Random Search"
      ],
      "metadata": {
        "id": "5J6jDD5QkCUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost + Random Search with Smaller LR and More Trees\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"===== FINE-TUNING PSEUDO-LABELING XGBOOST (Small LR + More Trees) =====\")\n",
        "\n",
        "# 1. Pseudo-labeling\n",
        "X_aug_xgb, y_aug_xgb = apply_pseudo_labeling(xgb, X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor, threshold=0.95)\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n",
        "    X_aug_xgb, y_aug_xgb, stratify=y_aug_xgb, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Column types\n",
        "categorical_cols = X_train_xgb.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = X_train_xgb.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "# 4. Preprocessor\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "    ('num', StandardScaler(), numerical_cols)\n",
        "])\n",
        "\n",
        "# 5. Pipeline\n",
        "xgb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
        "])\n",
        "\n",
        "# 6. New hyperparameter search space\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [300, 400, 500, 600, 800],\n",
        "    'classifier__max_depth': [3, 4, 5, 6],\n",
        "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
        "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'classifier__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    'classifier__gamma': [0, 0.05, 0.1],\n",
        "    'classifier__reg_alpha': [0, 0.01, 0.1],\n",
        "    'classifier__reg_lambda': [1, 1.5, 2.0]\n",
        "}\n",
        "\n",
        "# 7. RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 8. Fit\n",
        "random_search.fit(X_train_xgb, y_train_xgb)\n",
        "\n",
        "# 9. Predict\n",
        "best_xgb_pipeline = random_search.best_estimator_\n",
        "y_pred_xgb = best_xgb_pipeline.predict(X_test_xgb)\n",
        "\n",
        "# 10. Report\n",
        "print(\"\\n===== Best Parameters =====\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\n===== Best Cross-Validation Accuracy =====\")\n",
        "print(random_search.best_score_)\n",
        "\n",
        "print(\"\\n===== Test Set Classification Report =====\")\n",
        "print(classification_report(y_test_xgb, y_pred_xgb))\n",
        "\n",
        "# 11. Save model and best params\n",
        "save_dir = \"/content/models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(best_xgb_pipeline, os.path.join(save_dir, \"best_xgb_pipeline_v2.pkl\"))\n",
        "print(f\"Model saved to {save_dir}/best_xgb_pipeline_v2.pkl\")\n",
        "\n",
        "with open(os.path.join(save_dir, \"best_xgb_params_v2.json\"), \"w\") as f:\n",
        "    json.dump(random_search.best_params_, f, indent=4)\n",
        "print(f\"Best parameters saved to {save_dir}/best_xgb_params_v2.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHKBUsCocJE",
        "outputId": "44110218-c6fb-45f3-beab-24972341de21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== FINE-TUNING PSEUDO-LABELING XGBOOST (Small LR + More Trees) =====\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:59:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Parameters =====\n",
            "{'classifier__subsample': 0.8, 'classifier__reg_lambda': 2.0, 'classifier__reg_alpha': 0.1, 'classifier__n_estimators': 300, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.03, 'classifier__gamma': 0.1, 'classifier__colsample_bytree': 0.7}\n",
            "\n",
            "===== Best Cross-Validation Accuracy =====\n",
            "0.840089460441711\n",
            "\n",
            "===== Test Set Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.88      0.90        93\n",
            "           1       0.88      0.92      0.90        91\n",
            "\n",
            "    accuracy                           0.90       184\n",
            "   macro avg       0.90      0.90      0.90       184\n",
            "weighted avg       0.90      0.90      0.90       184\n",
            "\n",
            "Model saved to /content/models/best_xgb_pipeline_v2.pkl\n",
            "Best parameters saved to /content/models/best_xgb_params_v2.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:59:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Catboost Random Search Tuning"
      ],
      "metadata": {
        "id": "Ds61MnnAkMkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"===== TUNING PSEUDO-LABELING CATBOOST =====\")\n",
        "\n",
        "# 2. Define CatBoost model\n",
        "cat_model = CatBoostClassifier(\n",
        "    cat_features=cat_features_indices,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# 3. Parameter grid for CatBoost\n",
        "param_dist = {\n",
        "    'iterations': [300, 500, 700],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
        "    'depth': [4, 6, 8, 10],\n",
        "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'rsm': [0.7, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "# 4. RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=cat_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 5. Fit\n",
        "random_search.fit(X_train_cat, y_train_cat)\n",
        "\n",
        "# 6. Best model\n",
        "best_cat = random_search.best_estimator_\n",
        "\n",
        "# 7. Predict on test set\n",
        "y_pred_cat = best_cat.predict(X_test_cat)\n",
        "\n",
        "# 8. Evaluation\n",
        "print(\"\\n===== Best Parameters =====\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\n===== Best Cross-Validation Accuracy =====\")\n",
        "print(random_search.best_score_)\n",
        "\n",
        "print(\"\\n===== Test Set Classification Report =====\")\n",
        "print(classification_report(y_test_cat, y_pred_cat))\n",
        "\n",
        "# 9. Save model and params\n",
        "save_dir = \"/content/models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(best_cat, os.path.join(save_dir, \"best_cat_pipeline.pkl\"))\n",
        "print(f\"Model saved to {save_dir}/best_cat_pipeline.pkl\")\n",
        "\n",
        "with open(os.path.join(save_dir, \"best_cat_params.json\"), \"w\") as f:\n",
        "    json.dump(random_search.best_params_, f, indent=4)\n",
        "print(f\"Best parameters saved to {save_dir}/best_cat_params.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xh_sJz4qzrQ",
        "outputId": "3116535d-9939-4fea-c8a7-84296db6e9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== TUNING PSEUDO-LABELING CATBOOST =====\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Parameters =====\n",
            "{'subsample': 0.8, 'rsm': 1.0, 'learning_rate': 0.05, 'l2_leaf_reg': 7, 'iterations': 500, 'depth': 10}\n",
            "\n",
            "===== Best Cross-Validation Accuracy =====\n",
            "0.8412543099431552\n",
            "\n",
            "===== Test Set Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.85      0.87        93\n",
            "           1       0.85      0.90      0.88        90\n",
            "\n",
            "    accuracy                           0.87       183\n",
            "   macro avg       0.88      0.87      0.87       183\n",
            "weighted avg       0.88      0.87      0.87       183\n",
            "\n",
            "Model saved to /content/models/best_cat_pipeline.pkl\n",
            "Best parameters saved to /content/models/best_cat_params.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# Predict probabilities (make sure to use predict_proba)\n",
        "y_proba_xgb = best_xgb_pipeline.predict_proba(X_test_xgb)[:, 1]  # Take the probability of the positive class (1)\n",
        "\n",
        "# Compute False Positive Rate and True Positive Rate\n",
        "fpr, tpr, thresholds = roc_curve(y_test_xgb, y_proba_xgb)\n",
        "\n",
        "# Calculate AUC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'XGBoost (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', lw=1)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - XGBoost (Pseudo-Labeling Fine-Tuned)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ym04fjlauQwg",
        "outputId": "df92d142-2094-42e6-f8d0-c5ab367e850c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTCbQmmnFbZa",
        "outputId": "c6ccb3c6-a083-4156-a9c6-1a098d900075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tune with Optuna"
      ],
      "metadata": {
        "id": "NtX52RmVKGka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Used Optuna for Tuning\n",
        "\n",
        "Optuna was introduced into the tuning process for several reasons:\n",
        "\n",
        "- **Automated Hyperparameter Optimization (AutoML):**  \n",
        "  Optuna is considered a lightweight AutoML framework. It efficiently searches large and complex hyperparameter spaces using intelligent algorithms like Bayesian Optimization and pruning strategies.\n",
        "\n",
        "- **Efficiency and Flexibility:**  \n",
        "  Compared to traditional methods like Grid Search or Randomized Search, Optuna dynamically focuses trials on promising areas, significantly improving search efficiency without requiring an exhaustive number of experiments.\n",
        "\n",
        "- **Joint Tuning of Model Hyperparameters and Threshold:**  \n",
        "  One major advantage of Optuna is its ability to optimize both model hyperparameters and custom variables, such as the pseudo-labeling threshold.  \n",
        "  As previously discussed, different models showed varying sensitivities to pseudo-labeling thresholds.  \n",
        "  By tuning the threshold together with model parameters, Optuna allowed us to better adapt to the semi-supervised structure of our learning process.\n",
        "\n",
        "- **Ease of Integration with Semi-Supervised Learning:**  \n",
        "  Optuna’s flexible objective function made it straightforward to incorporate pseudo-labeling into the tuning loop, enabling smarter and more targeted optimization across both model training and pseudo-label selection."
      ],
      "metadata": {
        "id": "UPf9OUuKlFOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8dn9fkIKaFP",
        "outputId": "d38f3bbc-1289-4de1-e281-0d64d2827a14",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_base, X_val, y_train_base, y_val = train_test_split(\n",
        "    X_train, y_train, stratify=y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(\n",
        "    X_train_base, y_train_base, stratify=y_train_base, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Qiq95lcfKsz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def train_with_pseudo_pipeline(X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "                                threshold=0.95,\n",
        "                                max_depth=5, learning_rate=0.1,\n",
        "                                subsample=1.0, colsample_bytree=1.0,\n",
        "                                gamma=0, reg_alpha=0, reg_lambda=1,\n",
        "                                n_estimators=500):\n",
        "    \"\"\"\n",
        "    Train a Pseudo-Labeled XGBoost model with preprocessing pipeline and evaluate on validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split columns by type\n",
        "    categorical_cols = X_labeled.select_dtypes(include=['object']).columns.tolist()\n",
        "    numerical_cols = X_labeled.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "    # Preprocessing\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "        ('num', StandardScaler(), numerical_cols)\n",
        "    ])\n",
        "\n",
        "    # 1. Train base model on labeled data\n",
        "    base_xgb = XGBClassifier(\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        gamma=gamma,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        n_estimators=n_estimators,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    base_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', base_xgb)\n",
        "    ])\n",
        "\n",
        "    base_pipeline.fit(X_labeled, y_labeled)\n",
        "\n",
        "    # 2. Predict pseudo-labels\n",
        "    proba = base_pipeline.predict_proba(X_unlabeled)\n",
        "    confidence = np.max(proba, axis=1)\n",
        "    mask = confidence >= threshold\n",
        "    X_pseudo = X_unlabeled.iloc[mask]\n",
        "    y_pseudo = np.argmax(proba[mask], axis=1)\n",
        "\n",
        "    # 3. Merge pseudo-labeled and labeled data\n",
        "    X_train_aug = pd.concat([X_labeled, X_pseudo])\n",
        "    y_train_aug = np.concatenate([y_labeled, y_pseudo])\n",
        "\n",
        "    # 4. Re-train final model on augmented dataset\n",
        "    final_xgb = XGBClassifier(\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        gamma=gamma,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        n_estimators=n_estimators,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    final_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', final_xgb)\n",
        "    ])\n",
        "\n",
        "    final_pipeline.fit(X_train_aug, y_train_aug)\n",
        "\n",
        "    # 5. Evaluate on validation set\n",
        "    y_val_pred = final_pipeline.predict_proba(X_val)[:, 1]\n",
        "    auc = roc_auc_score(y_val, y_val_pred)\n",
        "\n",
        "    return auc, final_pipeline"
      ],
      "metadata": {
        "id": "u8ldk_t6GRdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def objective(trial):\n",
        "    threshold = trial.suggest_float(\"threshold\", 0.85, 0.99)\n",
        "    n_estimators = trial.suggest_categorical(\"n_estimators\", [300, 400, 500, 600, 800])\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 6)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.07)\n",
        "    subsample = trial.suggest_float(\"subsample\", 0.7, 1.0)\n",
        "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.7, 1.0)\n",
        "    gamma = trial.suggest_float(\"gamma\", 0, 0.1)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 0.1)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1.0, 2.0)\n",
        "\n",
        "    auc, _ = train_with_pseudo_pipeline(\n",
        "        X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "        threshold=threshold,\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        gamma=gamma,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        n_estimators=n_estimators\n",
        "    )\n",
        "    return auc"
      ],
      "metadata": {
        "id": "hX0Go19mKPjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "print(\"===== Best AUC =====\")\n",
        "print(study.best_value)\n",
        "\n",
        "print(\"===== Best Hyperparameters =====\")\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrHVgW01KfUG",
        "outputId": "805a0df4-d0cc-4f2e-86ec-bfe4e4e25adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-26 03:59:43,175] A new study created in memory with name: no-name-11cee0fb-1fe7-4f11-bf25-ed54271b0dcc\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:44,984] Trial 0 finished with value: 0.8537853785378537 and parameters: {'threshold': 0.884792427372296, 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.02362371605662158, 'subsample': 0.7874246925480611, 'colsample_bytree': 0.9518290845276118, 'gamma': 0.055124829792774534, 'reg_alpha': 0.07422459751024053, 'reg_lambda': 1.8534691729852995}. Best is trial 0 with value: 0.8537853785378537.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:47,181] Trial 1 finished with value: 0.8526852685268527 and parameters: {'threshold': 0.8942636197586529, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05589727706249057, 'subsample': 0.9873453613895622, 'colsample_bytree': 0.9191347191974368, 'gamma': 0.057304971909610515, 'reg_alpha': 0.06708600198983941, 'reg_lambda': 1.689707647667205}. Best is trial 0 with value: 0.8537853785378537.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:47,597] Trial 2 finished with value: 0.8536853685368537 and parameters: {'threshold': 0.9879877284015514, 'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.03829518915601682, 'subsample': 0.9514312790468873, 'colsample_bytree': 0.7009982241612134, 'gamma': 0.036931335747659215, 'reg_alpha': 0.003544716698781614, 'reg_lambda': 1.2559236187360536}. Best is trial 0 with value: 0.8537853785378537.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:48,413] Trial 3 finished with value: 0.8440844084408441 and parameters: {'threshold': 0.8710860445645224, 'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.03245669251735964, 'subsample': 0.9837070023936223, 'colsample_bytree': 0.9402022448115233, 'gamma': 0.028386943322510228, 'reg_alpha': 0.0841406200089321, 'reg_lambda': 1.603781358177504}. Best is trial 0 with value: 0.8537853785378537.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:48,901] Trial 4 finished with value: 0.8606860686068606 and parameters: {'threshold': 0.9635866023083535, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01802554343921212, 'subsample': 0.9535786857985756, 'colsample_bytree': 0.9158367171292215, 'gamma': 0.04025821884965744, 'reg_alpha': 0.08584900771178185, 'reg_lambda': 1.72597042597856}. Best is trial 4 with value: 0.8606860686068606.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:49,260] Trial 5 finished with value: 0.8523852385238524 and parameters: {'threshold': 0.9324060409069062, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05999606748227722, 'subsample': 0.7156626221660858, 'colsample_bytree': 0.8167637603183904, 'gamma': 0.06108065709489585, 'reg_alpha': 0.031640060149912844, 'reg_lambda': 1.8073165680606074}. Best is trial 4 with value: 0.8606860686068606.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:49,905] Trial 6 finished with value: 0.8486848684868487 and parameters: {'threshold': 0.8509613539159504, 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.06178335718330976, 'subsample': 0.772725592678698, 'colsample_bytree': 0.932722097755603, 'gamma': 0.04531826853909237, 'reg_alpha': 0.01483703667029116, 'reg_lambda': 1.0542418667150992}. Best is trial 4 with value: 0.8606860686068606.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:50,320] Trial 7 finished with value: 0.8566856685668567 and parameters: {'threshold': 0.8899830298910812, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.040576970208457644, 'subsample': 0.8223385966594666, 'colsample_bytree': 0.8651327413028503, 'gamma': 0.067585486233859, 'reg_alpha': 0.053673209651320865, 'reg_lambda': 1.8315765296777904}. Best is trial 4 with value: 0.8606860686068606.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:50,599] Trial 8 finished with value: 0.8661866186618662 and parameters: {'threshold': 0.8793774661626955, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.05691621401877822, 'subsample': 0.7606726941481972, 'colsample_bytree': 0.7693262553418311, 'gamma': 0.005933720967610323, 'reg_alpha': 0.06677034457614112, 'reg_lambda': 1.1034614521182116}. Best is trial 8 with value: 0.8661866186618662.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:50,936] Trial 9 finished with value: 0.8709870987098709 and parameters: {'threshold': 0.9469173932804776, 'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.029033629848271, 'subsample': 0.7564770579555065, 'colsample_bytree': 0.9403937636703612, 'gamma': 0.09884308803988308, 'reg_alpha': 0.04852431057696476, 'reg_lambda': 1.7397938714612216}. Best is trial 9 with value: 0.8709870987098709.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:51,322] Trial 10 finished with value: 0.8771877187718772 and parameters: {'threshold': 0.9301020904673649, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.010461335154923989, 'subsample': 0.8731919040369485, 'colsample_bytree': 0.9967170928002269, 'gamma': 0.09797333836395301, 'reg_alpha': 0.03755205592606774, 'reg_lambda': 1.3979769081211268}. Best is trial 10 with value: 0.8771877187718772.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:51,733] Trial 11 finished with value: 0.8754875487548754 and parameters: {'threshold': 0.9304695625100781, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.010675724482002567, 'subsample': 0.8713890130862837, 'colsample_bytree': 0.9961732819961862, 'gamma': 0.09957451059945471, 'reg_alpha': 0.03583392270484593, 'reg_lambda': 1.4308160469423263}. Best is trial 10 with value: 0.8771877187718772.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:52,135] Trial 12 finished with value: 0.8777877787778778 and parameters: {'threshold': 0.9179763306841262, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.01037274571738057, 'subsample': 0.8884719473086485, 'colsample_bytree': 0.9872081859890994, 'gamma': 0.09645513991942949, 'reg_alpha': 0.030297144809065334, 'reg_lambda': 1.3815623782143183}. Best is trial 12 with value: 0.8777877787778778.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:52,534] Trial 13 finished with value: 0.8796879687968796 and parameters: {'threshold': 0.911962456301616, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.010015747039304563, 'subsample': 0.8957288089808539, 'colsample_bytree': 0.9751202284164399, 'gamma': 0.0801807687454055, 'reg_alpha': 0.02659336926000543, 'reg_lambda': 1.389155969629045}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:52,938] Trial 14 finished with value: 0.8727872787278728 and parameters: {'threshold': 0.9084325126587135, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.019330162929163557, 'subsample': 0.9078401682037858, 'colsample_bytree': 0.870319518973958, 'gamma': 0.07812069626504456, 'reg_alpha': 0.019077117704299873, 'reg_lambda': 1.2810219714815845}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:53,273] Trial 15 finished with value: 0.8661866186618662 and parameters: {'threshold': 0.9117160619390344, 'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.04956972382694784, 'subsample': 0.9161081998851255, 'colsample_bytree': 0.9819926952639119, 'gamma': 0.08188434589327041, 'reg_alpha': 0.01948801743410428, 'reg_lambda': 1.5507507622384715}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:53,937] Trial 16 finished with value: 0.8613861386138614 and parameters: {'threshold': 0.9592283353539444, 'n_estimators': 800, 'max_depth': 4, 'learning_rate': 0.01735881306305821, 'subsample': 0.833550375839719, 'colsample_bytree': 0.8965560775753632, 'gamma': 0.08249083560794204, 'reg_alpha': 0.00022474489092079095, 'reg_lambda': 1.262245305343523}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:54,362] Trial 17 finished with value: 0.8531853185318532 and parameters: {'threshold': 0.9059360951404799, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.06947917479316712, 'subsample': 0.9021220407321123, 'colsample_bytree': 0.8225123405032911, 'gamma': 0.07365563128360944, 'reg_alpha': 0.05084089940699115, 'reg_lambda': 1.4104289062747382}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:54,752] Trial 18 finished with value: 0.8721872187218722 and parameters: {'threshold': 0.9458150863094014, 'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.026603479828179205, 'subsample': 0.8651645038641083, 'colsample_bytree': 0.9694059437234137, 'gamma': 0.0895363202689133, 'reg_alpha': 0.028699162136969318, 'reg_lambda': 1.9534967208931042}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:55,179] Trial 19 finished with value: 0.8713871387138713 and parameters: {'threshold': 0.8665056585657531, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.01430830234281142, 'subsample': 0.8140878998024255, 'colsample_bytree': 0.8893750164847856, 'gamma': 0.0883062768002168, 'reg_alpha': 0.09811213798101787, 'reg_lambda': 1.163447197125257}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:55,938] Trial 20 finished with value: 0.8454845484548454 and parameters: {'threshold': 0.9192515963579811, 'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.03516653045207577, 'subsample': 0.9367710405725873, 'colsample_bytree': 0.9639507366886076, 'gamma': 0.06593454210609638, 'reg_alpha': 0.0110467195731554, 'reg_lambda': 1.5155732569700913}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:56,371] Trial 21 finished with value: 0.8758875887588758 and parameters: {'threshold': 0.9272283147291351, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.011446430446951386, 'subsample': 0.8816612747017997, 'colsample_bytree': 0.9922877127666927, 'gamma': 0.0922634276436007, 'reg_alpha': 0.04143719420246967, 'reg_lambda': 1.4223381010711138}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:56,794] Trial 22 finished with value: 0.8687868786878689 and parameters: {'threshold': 0.8998804100149143, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.021566994802018638, 'subsample': 0.8545886891578085, 'colsample_bytree': 0.9994823650153294, 'gamma': 0.09442556074458835, 'reg_alpha': 0.024919620885147736, 'reg_lambda': 1.3628012256710034}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:57,201] Trial 23 finished with value: 0.8790879087908791 and parameters: {'threshold': 0.942640716529333, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.010632573992067768, 'subsample': 0.9058245849066566, 'colsample_bytree': 0.9620188022757034, 'gamma': 0.08581764889865746, 'reg_alpha': 0.04122768413454175, 'reg_lambda': 1.3497725529461768}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 03:59:58,859] Trial 24 finished with value: 0.8725872587258726 and parameters: {'threshold': 0.9505447648710095, 'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.01545342910911075, 'subsample': 0.923638224727912, 'colsample_bytree': 0.9600371409761823, 'gamma': 0.08409656690230098, 'reg_alpha': 0.04492472385977008, 'reg_lambda': 1.3212686476095288}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:59:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 04:00:00,695] Trial 25 finished with value: 0.8532853285328532 and parameters: {'threshold': 0.983303130408238, 'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.04556791400356569, 'subsample': 0.8974384081999706, 'colsample_bytree': 0.9140589551801471, 'gamma': 0.0708758993609989, 'reg_alpha': 0.05789305819288339, 'reg_lambda': 1.1593603859478456}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 04:00:01,390] Trial 26 finished with value: 0.8625862586258627 and parameters: {'threshold': 0.9218167499474992, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.024523100970450823, 'subsample': 0.9654939523981629, 'colsample_bytree': 0.9645964000711301, 'gamma': 0.07589719756255842, 'reg_alpha': 0.02548669645494112, 'reg_lambda': 1.6217284641518233}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 04:00:01,806] Trial 27 finished with value: 0.8716871687168717 and parameters: {'threshold': 0.9716829104651046, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.01487507812736049, 'subsample': 0.8432690297766323, 'colsample_bytree': 0.823599037961764, 'gamma': 0.088903756457211, 'reg_alpha': 0.03392624190936976, 'reg_lambda': 1.4738375085401503}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 04:00:02,142] Trial 28 finished with value: 0.8694869486948694 and parameters: {'threshold': 0.9421019481930962, 'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.029942294599257145, 'subsample': 0.8864575510601742, 'colsample_bytree': 0.7743385010989827, 'gamma': 0.007625915515196202, 'reg_alpha': 0.007332570943369435, 'reg_lambda': 1.19925454326308}. Best is trial 13 with value: 0.8796879687968796.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-26 04:00:02,697] Trial 29 finished with value: 0.8635863586358635 and parameters: {'threshold': 0.9135497143070987, 'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.021267455419505955, 'subsample': 0.8074107271963471, 'colsample_bytree': 0.9481464258030502, 'gamma': 0.05048365973313795, 'reg_alpha': 0.05899646731114504, 'reg_lambda': 1.3490215424658232}. Best is trial 13 with value: 0.8796879687968796.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Best AUC =====\n",
            "0.8796879687968796\n",
            "===== Best Hyperparameters =====\n",
            "{'threshold': 0.911962456301616, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.010015747039304563, 'subsample': 0.8957288089808539, 'colsample_bytree': 0.9751202284164399, 'gamma': 0.0801807687454055, 'reg_alpha': 0.02659336926000543, 'reg_lambda': 1.389155969629045}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "_, best_pipeline = train_with_pseudo_pipeline(\n",
        "    X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "    **study.best_params\n",
        ")\n",
        "\n",
        "y_test_pred = best_pipeline.predict(X_test)\n",
        "\n",
        "print(\"\\n===== Test Set Classification Report =====\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLARc-SfMhgV",
        "outputId": "a53efb6d-2268-4c96-a00b-083fdd81471b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Test Set Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.78      0.84       124\n",
            "           1       0.81      0.92      0.86       127\n",
            "\n",
            "    accuracy                           0.85       251\n",
            "   macro avg       0.86      0.85      0.85       251\n",
            "weighted avg       0.86      0.85      0.85       251\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-train the best model\n",
        "best_auc, best_pipeline = train_with_pseudo_pipeline(\n",
        "    X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "    **study.best_params\n",
        ")\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "save_dir = \"/content/models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(best_pipeline, os.path.join(save_dir, \"best_xgb_pipeline_pseudo.pkl\"))\n",
        "print(f\"Model saved to {save_dir}/best_xgb_pipeline_pseudo.pkl\")\n",
        "\n",
        "with open(os.path.join(save_dir, \"best_xgb_params_pseudo.json\"), \"w\") as f:\n",
        "    json.dump(study.best_params, f, indent=4)\n",
        "print(f\"Best parameters saved to {save_dir}/best_xgb_params_pseudo.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpbDMyZZKg5X",
        "outputId": "c90b2182-3124-47a4-bc2f-aa045a168154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:00:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/models/best_xgb_pipeline_pseudo.pkl\n",
            "Best parameters saved to /content/models/best_xgb_params_pseudo.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Optuna"
      ],
      "metadata": {
        "id": "EZHpENsQthX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def train_rf_with_pseudo_pipeline(X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "                                   threshold=0.9,\n",
        "                                   n_estimators=200, max_depth=10,\n",
        "                                   min_samples_split=2, min_samples_leaf=1,\n",
        "                                   max_features='sqrt', bootstrap=True):\n",
        "    \"\"\"\n",
        "    Train a Pseudo-Labeled Random Forest model with preprocessing pipeline and evaluate on validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Apply pseudo-labeling\n",
        "    X_aug_rf, y_aug_rf = apply_pseudo_labeling(\n",
        "        RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            max_features=max_features,\n",
        "            bootstrap=bootstrap,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "        X_labeled, y_labeled, X_unlabeled, preprocessor=preprocessor, threshold=threshold\n",
        "    )\n",
        "\n",
        "    # 2. Split augmented data into train/test\n",
        "    X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(\n",
        "        X_aug_rf, y_aug_rf, stratify=y_aug_rf, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 3. Build pipeline\n",
        "    rf_pipeline = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            max_features=max_features,\n",
        "            bootstrap=bootstrap,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 4. Fit on training\n",
        "    rf_pipeline.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "    # 5. Evaluate on validation\n",
        "    y_val_pred = rf_pipeline.predict_proba(X_val)[:, 1]\n",
        "    auc = roc_auc_score(y_val, y_val_pred)\n",
        "\n",
        "    return auc, rf_pipeline\n",
        "\n",
        "# Objective function used to call Optuna\n",
        "def rf_objective(trial):\n",
        "    threshold = trial.suggest_float(\"threshold\", 0.85, 0.99)\n",
        "    n_estimators = trial.suggest_categorical(\"n_estimators\", [200, 300, 400, 500, 800])\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n",
        "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
        "    max_features = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2', 0.5, 0.8])\n",
        "    bootstrap = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
        "\n",
        "    auc, _ = train_rf_with_pseudo_pipeline(\n",
        "        X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "        threshold=threshold,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        bootstrap=bootstrap\n",
        "    )\n",
        "\n",
        "    return auc\n",
        "\n",
        "study_rf = optuna.create_study(direction=\"maximize\")\n",
        "study_rf.optimize(rf_objective, n_trials=30)\n",
        "\n",
        "print(\"===== Best AUC (Random Forest) =====\")\n",
        "print(study_rf.best_value)\n",
        "\n",
        "print(\"===== Best Hyperparameters (Random Forest) =====\")\n",
        "print(study_rf.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOXA5QEnMHco",
        "outputId": "4fa8aebc-ef34-480d-9610-21bf9af02f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-26 06:26:29,998] A new study created in memory with name: no-name-855c4779-5a71-429f-b7b8-f9753d49f2c8\n",
            "[I 2025-04-26 06:26:32,520] Trial 0 finished with value: 0.8656865686568657 and parameters: {'threshold': 0.897940033543965, 'n_estimators': 400, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 0.8656865686568657.\n",
            "[I 2025-04-26 06:26:34,513] Trial 1 finished with value: 0.8653865386538654 and parameters: {'threshold': 0.9488273841796223, 'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 0.8656865686568657.\n",
            "[I 2025-04-26 06:26:36,334] Trial 2 finished with value: 0.8622862286228623 and parameters: {'threshold': 0.9581372311441481, 'n_estimators': 200, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': True}. Best is trial 0 with value: 0.8656865686568657.\n",
            "[I 2025-04-26 06:26:38,178] Trial 3 finished with value: 0.8594359435943595 and parameters: {'threshold': 0.9865559283747762, 'n_estimators': 200, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 0.8, 'bootstrap': True}. Best is trial 0 with value: 0.8656865686568657.\n",
            "[I 2025-04-26 06:26:39,928] Trial 4 finished with value: 0.8616861686168616 and parameters: {'threshold': 0.9891853312148656, 'n_estimators': 500, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': False}. Best is trial 0 with value: 0.8656865686568657.\n",
            "[I 2025-04-26 06:26:41,093] Trial 5 finished with value: 0.8733873387338734 and parameters: {'threshold': 0.9003234363160211, 'n_estimators': 300, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:45,489] Trial 6 finished with value: 0.854985498549855 and parameters: {'threshold': 0.9421072095888731, 'n_estimators': 800, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.5, 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:47,229] Trial 7 finished with value: 0.8487848784878487 and parameters: {'threshold': 0.9342738577322991, 'n_estimators': 300, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.5, 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:49,871] Trial 8 finished with value: 0.8564356435643564 and parameters: {'threshold': 0.922333806090853, 'n_estimators': 300, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 0.8, 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:52,426] Trial 9 finished with value: 0.8653865386538654 and parameters: {'threshold': 0.9681561843984252, 'n_estimators': 400, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 0.5, 'bootstrap': False}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:53,685] Trial 10 finished with value: 0.8433843384338433 and parameters: {'threshold': 0.8535856642319897, 'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:55,286] Trial 11 finished with value: 0.8674867486748674 and parameters: {'threshold': 0.8888169806172069, 'n_estimators': 400, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:56,872] Trial 12 finished with value: 0.8683868386838683 and parameters: {'threshold': 0.8836172569506054, 'n_estimators': 400, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:26:58,819] Trial 13 finished with value: 0.8725872587258726 and parameters: {'threshold': 0.8864423674597897, 'n_estimators': 500, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 5 with value: 0.8733873387338734.\n",
            "[I 2025-04-26 06:27:00,406] Trial 14 finished with value: 0.8736873687368737 and parameters: {'threshold': 0.8593411822051251, 'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:01,997] Trial 15 finished with value: 0.8714871487148714 and parameters: {'threshold': 0.8523886330682855, 'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:06,579] Trial 16 finished with value: 0.853985398539854 and parameters: {'threshold': 0.869866313292589, 'n_estimators': 800, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:07,852] Trial 17 finished with value: 0.8498849884988499 and parameters: {'threshold': 0.9045650077784391, 'n_estimators': 300, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:09,791] Trial 18 finished with value: 0.8573857385738574 and parameters: {'threshold': 0.8703546746514633, 'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:10,825] Trial 19 finished with value: 0.8727872787278728 and parameters: {'threshold': 0.9139043152665336, 'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:12,586] Trial 20 finished with value: 0.8682868286828683 and parameters: {'threshold': 0.8761352251601061, 'n_estimators': 500, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:13,638] Trial 21 finished with value: 0.8693869386938693 and parameters: {'threshold': 0.9138212271991809, 'n_estimators': 300, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:14,678] Trial 22 finished with value: 0.8727872787278728 and parameters: {'threshold': 0.9088127634866475, 'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:15,805] Trial 23 finished with value: 0.8596859685968598 and parameters: {'threshold': 0.9250639132508474, 'n_estimators': 300, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 14 with value: 0.8736873687368737.\n",
            "[I 2025-04-26 06:27:17,224] Trial 24 finished with value: 0.8767876787678767 and parameters: {'threshold': 0.8940030778309433, 'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 24 with value: 0.8767876787678767.\n",
            "[I 2025-04-26 06:27:20,930] Trial 25 finished with value: 0.869086908690869 and parameters: {'threshold': 0.89808793163883, 'n_estimators': 800, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 24 with value: 0.8767876787678767.\n",
            "[I 2025-04-26 06:27:22,644] Trial 26 finished with value: 0.8642864286428643 and parameters: {'threshold': 0.8640216141095666, 'n_estimators': 500, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False}. Best is trial 24 with value: 0.8767876787678767.\n",
            "[I 2025-04-26 06:27:24,597] Trial 27 finished with value: 0.8355335533553355 and parameters: {'threshold': 0.861489457179804, 'n_estimators': 300, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.5, 'bootstrap': False}. Best is trial 24 with value: 0.8767876787678767.\n",
            "[I 2025-04-26 06:27:26,626] Trial 28 finished with value: 0.8552855285528552 and parameters: {'threshold': 0.8958043440883521, 'n_estimators': 300, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': True}. Best is trial 24 with value: 0.8767876787678767.\n",
            "[I 2025-04-26 06:27:28,626] Trial 29 finished with value: 0.8620862086208619 and parameters: {'threshold': 0.876197632171757, 'n_estimators': 500, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True}. Best is trial 24 with value: 0.8767876787678767.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Best AUC (Random Forest) =====\n",
            "0.8767876787678767\n",
            "===== Best Hyperparameters (Random Forest) =====\n",
            "{'threshold': 0.8940030778309433, 'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "auc_best, best_rf_pipeline = train_rf_with_pseudo_pipeline(\n",
        "    X_labeled, y_labeled, X_unlabeled, y_val, X_val,\n",
        "    threshold=0.8940030778309433,\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=3,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=False\n",
        ")\n",
        "\n",
        "y_val_pred_label = best_rf_pipeline.predict(X_val)\n",
        "\n",
        "print(\"===== Classification Report (Best RF Model) =====\")\n",
        "print(classification_report(y_val, y_val_pred_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wCu90MCxqq2",
        "outputId": "2299b759-dde3-410c-e41f-5db2372c0602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Classification Report (Best RF Model) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.73      0.81        99\n",
            "           1       0.78      0.93      0.85       101\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.84      0.83      0.83       200\n",
            "weighted avg       0.84      0.83      0.83       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Optuna Tuning Results Analysis\n",
        "\n",
        "#### **XGBoost Tuning with Optuna**\n",
        "\n",
        "#### Tuning Approach\n",
        "- We used Optuna to simultaneously tune:\n",
        "  - Pseudo-labeling `threshold` (range: 0.85–0.99)\n",
        "  - XGBoost hyperparameters, including `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `gamma`, `reg_alpha`, and `reg_lambda`.\n",
        "- The objective was to maximize **AUC** on the validation set.\n",
        "\n",
        "#### Tuning\n",
        "- After 30 trials, the Optuna-tuned XGBoost model showed slight improvements compared to the initial pseudo-labeling baseline.\n",
        "- However, the Optuna-tuned model **did not significantly outperform** the version tuned with RandomizedSearchCV.\n",
        "- Given the comparable results and better reproducibility of the RandomizedSearchCV-tuned model, we retained that version for final deployment.\n",
        "\n",
        "\n",
        "### **Random Forest Tuning with Optuna**\n",
        "\n",
        "#### Tuning Approach\n",
        "- Optuna was also applied to tune Random Forest hyperparameters, including `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, and `bootstrap`.\n",
        "\n",
        "#### Tuning Outcome\n",
        "- Optuna tuning for Random Forest **underperformed** compared to the RandomizedSearchCV results.\n",
        "- Validation accuracy slightly decreased, and model generalization was less stable.\n",
        "- As a result, the Random Forest model tuned with RandomizedSearchCV was preferred.\n",
        "\n",
        "\n",
        "### Final Insights from Optuna Tuning\n",
        "\n",
        "- **Optuna provided an efficient framework for tuning complex models and thresholds simultaneously**, especially in the context of semi-supervised learning.\n",
        "- However, **in practice**, for both XGBoost and Random Forest:\n",
        "  - **RandomizedSearchCV produced better or more stable models**.\n",
        "  - **Optuna tuning results were not sufficient to replace existing RandomizedSearchCV–tuned models** in the final selection.\n",
        "- Nonetheless, Optuna remains a valuable tool for situations where more complex joint optimization is required, or where search spaces are very large and traditional randomized coverage becomes inefficient.\n"
      ],
      "metadata": {
        "id": "jjBpHggjlm_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Optuna Did Not Outperform RandomizedSearchCV for Random Forest Tuning\n",
        "\n",
        "#### Observations\n",
        "- **RandomizedSearchCV** achieved better validation results for Random Forest.\n",
        "- **Optuna tuning** slightly underperformed compared to RandomizedSearchCV.\n",
        "- **This is not unusual**, especially for robust models like Random Forests trained on pseudo-labeled data.\n",
        "\n",
        "\n",
        "### Core Reasons\n",
        "\n",
        "#### 1. **Exploration vs Coverage**\n",
        "- **RandomizedSearchCV** performs **uniform random sampling** over the entire predefined hyperparameter space.\n",
        "- **Optuna** uses **sequential optimization** (similar to Bayesian Optimization) and tends to focus on local promising areas.\n",
        "- If the initial search space is already well-designed, **RandomizedSearchCV can more easily \"catch\" good combinations** by global random coverage.\n",
        "- **Optuna**, while smarter in theory, may miss some globally optimal regions if the search is early-stopped or stuck in a local optimum.\n",
        "\n",
        "#### 2. **Random Forest is Naturally Stable**\n",
        "- Random Forest is a **robust, low-variance model**.\n",
        "- Its performance is **less sensitive** to hyperparameters compared to XGBoost or CatBoost.\n",
        "- **RandomizedSearchCV is often sufficient** to sweep the key hyperparameter combinations without fine-grained optimization.\n",
        "- **Optuna’s fine search** offers **less marginal gain** in this context.\n",
        "\n",
        "\n",
        "#### 3. **Few Optuna Trials**\n",
        "- If `n_trials` for Optuna is small (e.g., 30–50), it may not have enough chance to fully explore the space. But since our dataset is small, it's not ideal to increase n_trials to 100 due to high risk of overfitting.\n",
        "- RandomizedSearchCV covers space **uniformly at once**, while **Optuna needs multiple rounds to refine** its search.\n",
        "- **With small trial budgets**, RandomizedSearchCV often achieves better performance simply due to better global coverage.\n",
        "\n",
        "\n",
        "#### 4. **Noise from Pseudo-Labeling**\n",
        "- The dataset involves **pseudo-labeled samples**, which introduces inherent noise.\n",
        "- Hyperparameter tuning on noisy labels has **higher variance** and randomness.\n",
        "- RandomizedSearchCV, by sampling globally, is **more robust to noisy search spaces**.\n",
        "- **Optuna**, focusing narrowly on promising regions, may overfit to validation noise.\n",
        "\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Aspect | RandomizedSearchCV | Optuna |\n",
        "|:---|:---|:---|\n",
        "| Suitable Models | Stable models (RF) | Highly sensitive models (XGB, CatBoost) |\n",
        "| Search Strategy | Global random sampling | Sequential local optimization |\n",
        "| Performance with Few Trials | Good | Risky (may get stuck) |\n",
        "| Sensitivity to Pseudo-Label Noise | Low | High |\n",
        "| Recommended Use Case | Baseline tuning, quick sweeps | Deep fine-tuning with large trial budgets |\n",
        "\n",
        "\n",
        "### Practical Takeaways\n",
        "- For **Random Forest on pseudo-labeled data**, **RandomizedSearchCV with a good grid is enough**.\"\n",
        "- For **complex models** like **XGBoost**, **CatBoost**, or **Ensembles**, **Optuna is very valuable** when we have enough computation budget, but the size of our dataset is suitable for small n_trials which can reduce risk of overfitting."
      ],
      "metadata": {
        "id": "X87FrQSL0Hf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Search Revalidation Check"
      ],
      "metadata": {
        "id": "9XqQVhr1bQNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Random Search Revalidation\n",
        "\n",
        "Hyperparameter optimization is a crucial part of model development, and it's widely believed that **Optuna**, with its Bayesian optimization logic, should outperform **Random Search**, which is inherently stochastic. However, in this project, the **Random Search–tuned XGBoost model achieved superior classification performance** on the test set—despite being the simpler method.\n",
        "\n",
        "This raised an important question:\n",
        "**Did Random Search truly find a robust parameter configuration, or was it just a lucky hit on a specific data split?**\n",
        "To answer this, we conducted a systematic revalidation.\n",
        "\n",
        "### What Was Done\n",
        "\n",
        "We rebuilt the XGBoost pipeline using the exact parameters returned by Random Search, and then applied **5-fold cross-validation** on the full training set and evaluated the refitted model on the previously untouched test set."
      ],
      "metadata": {
        "id": "I6-jMufUWAxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Import packages ===\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "print(\"===== FINAL MODEL WITH BEST PARAMETERS =====\")\n",
        "\n",
        "# 1. Preprocessing\n",
        "categorical_cols = X_train_xgb.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = X_train_xgb.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "    ('num', StandardScaler(), numerical_cols)\n",
        "])\n",
        "\n",
        "# 2. Final XGBoost model with best parameters\n",
        "final_xgb = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    reg_lambda=2.0,\n",
        "    reg_alpha=0.1,\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.03,\n",
        "    gamma=0.1,\n",
        "    colsample_bytree=0.7\n",
        ")\n",
        "\n",
        "# 3. Final Pipeline\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', final_xgb)\n",
        "])\n",
        "\n",
        "# 4. Cross-Validation to re-check stability\n",
        "cv_scores = cross_val_score(final_pipeline, X_train_xgb, y_train_xgb, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"\\n===== Cross-Validation Result =====\")\n",
        "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Std CV Accuracy: {cv_scores.std():.4f}\")\n",
        "\n",
        "# 5. Train on full training set\n",
        "final_pipeline.fit(X_train_xgb, y_train_xgb)\n",
        "\n",
        "# 6. Predict on test set\n",
        "y_pred_final = final_pipeline.predict(X_test_xgb)\n",
        "\n",
        "# 7. Final Test Set Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"\\n===== Final Test Set Classification Report =====\")\n",
        "print(classification_report(y_test_xgb, y_pred_final))\n",
        "\n",
        "# 8. Save final model\n",
        "save_dir = \"/content/models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(final_pipeline, os.path.join(save_dir, \"final_xgb_pipeline.pkl\"))\n",
        "print(f\"Final model saved to {save_dir}/final_xgb_pipeline.pkl\")"
      ],
      "metadata": {
        "id": "xb1Sk67PyqiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95187f21-9373-47ce-a147-c2b18753b8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== FINAL MODEL WITH BEST PARAMETERS =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:03:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:03:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:03:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:03:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:03:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:03:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Cross-Validation Result =====\n",
            "Mean CV Accuracy: 0.8401\n",
            "Std CV Accuracy: 0.0259\n",
            "\n",
            "===== Final Test Set Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.88      0.90        93\n",
            "           1       0.88      0.92      0.90        91\n",
            "\n",
            "    accuracy                           0.90       184\n",
            "   macro avg       0.90      0.90      0.90       184\n",
            "weighted avg       0.90      0.90      0.90       184\n",
            "\n",
            "Final model saved to /content/models/final_xgb_pipeline.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Preprocessing\n",
        "categorical_cols = X_train_xgb.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = X_train_xgb.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "    ('num', StandardScaler(), numerical_cols)\n",
        "])\n",
        "\n",
        "# 2. Final XGBoost model with best parameters\n",
        "final_xgb = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    reg_lambda=2.0,\n",
        "    reg_alpha=0.1,\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.03,\n",
        "    gamma=0.1,\n",
        "    colsample_bytree=0.7\n",
        ")\n",
        "\n",
        "# 3. Final Pipeline\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', final_xgb)\n",
        "])\n",
        "\n",
        "# 4. Cross-Validation to re-check stability\n",
        "cv_scores = cross_val_score(final_pipeline, X_train_xgb, y_train_xgb, cv=5, scoring='accuracy')\n",
        "\n",
        "# Accuracy of Each Accuracy\n",
        "for i, score in enumerate(cv_scores):\n",
        "    print(f\"Fold {i+1} Accuracy: {score:.4f}\")\n",
        "\n",
        "# Print Result\n",
        "print(\"\\n===== Cross-Validation Summary =====\")\n",
        "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Std CV Accuracy: {np.std(cv_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMLezcf4bPLE",
        "outputId": "7667ca19-c670-4bc2-a205-234708ecb3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:32:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:32:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:32:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:32:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Accuracy: 0.8776\n",
            "Fold 2 Accuracy: 0.8571\n",
            "Fold 3 Accuracy: 0.8082\n",
            "Fold 4 Accuracy: 0.8425\n",
            "Fold 5 Accuracy: 0.8151\n",
            "\n",
            "===== Cross-Validation Summary =====\n",
            "Mean CV Accuracy: 0.8401\n",
            "Std CV Accuracy: 0.0259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:32:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search Revalidation Analysis\n",
        "\n",
        "### What We Found\n",
        "\n",
        "The model’s **cross-validation mean accuracy was 0.8401** with a standard deviation of just **±0.0259**, showing strong **consistency across folds**.\n",
        "Performance remained above **80% in every fold**, indicating **low sensitivity to data splits**.\n",
        "\n",
        "On the held-out test set, the model achieved an **accuracy of 0.90**, with **F1 scores of 0.90 for both classes**, and recall reaching **0.92** for the positive class. These results were well-aligned with CV performance and confirmed the model's ability to generalize.\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "Despite its randomness, **Random Search did not merely stumble upon a good solution**. It successfully explored a meaningful region of the hyperparameter space and returned a configuration that is **stable, generalizable, and superior to the Optuna result in this case**.\n",
        "\n",
        "The revalidation confirms that our selected parameters are not the product of noise, but rather reflect an effective fit to the structure of the data—**even better than what more sophisticated tuning methods produced**."
      ],
      "metadata": {
        "id": "CaFJmmdFVfXy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7n5PhLE3RoLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}